"""äº‘å­˜å‚¨ç®¡ç†å™¨"""
import streamlit as st
import pandas as pd
import os
import json
import hashlib
import mimetypes
import base64
import io
import time
import sqlite3
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import zipfile
import shutil
from pathlib import Path
import requests
from PIL import Image
import re
import numpy as np
from collections import Counter
import jieba
import jieba.analyse
import matplotlib.pyplot as plt
import seaborn as sns
import contextlib

# å°è¯•å¯¼å…¥tools.generate_reportï¼ˆå¯é€‰ï¼‰
try:
    from tools.generate_report import SmartAnalysisGenerator
    SMART_REPORT_AVAILABLE = True
except ImportError:
    SMART_REPORT_AVAILABLE = False

from config.settings import INDUSTRY_KEYWORDS, INDUSTRY_ENGLISH_MAPPING
from utils.dependencies import (
    PDF_AVAILABLE, OCR_AVAILABLE, ML_AVAILABLE, 
    TRANSFORMERS_AVAILABLE, OPENAI_AVAILABLE,
    TESSERACT_AVAILABLE
)

# å¯¼å…¥PDFæ”¯æŒåº“
if PDF_AVAILABLE:
    try:
        import fitz  # PyMuPDF
    except ImportError:
        fitz = None
else:
    fitz = None

# å¯¼å…¥OCRæ”¯æŒåº“ - ä»…ä½¿ç”¨Tesseract OCR
if TESSERACT_AVAILABLE:
    try:
        import pytesseract
        from PIL import Image
    except ImportError:
        pytesseract = None
        Image = None
else:
    pytesseract = None
    Image = None

class CloudStorageManager:
    def __init__(self):
        # äº‘éƒ¨ç½²é…ç½®
        import os
        self.is_cloud_deployment = os.getenv('STREAMLIT_SERVER_PORT') is not None

        if self.is_cloud_deployment:
            # äº‘éƒ¨ç½²ï¼šä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨
            self.storage_dir = Path("/tmp/cloud_storage")
            self.cache_dir = Path("/tmp/local_cache")
            self.ai_analysis_dir = Path("/tmp/ai_analysis")
        else:
            # æœ¬åœ°éƒ¨ç½²ï¼šä½¿ç”¨å½“å‰ç›®å½•
            self.storage_dir = Path("cloud_storage")
            self.cache_dir = Path("local_cache")
            self.ai_analysis_dir = Path("ai_analysis")

        # åˆ›å»ºç›®å½•
        self.storage_dir.mkdir(exist_ok=True)
        self.cache_dir.mkdir(exist_ok=True)
        self.ai_analysis_dir.mkdir(exist_ok=True)

        # å°†è·¯å¾„è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç¡®ä¿åœ¨Windowsä¸Šæ­£å¸¸å·¥ä½œ
        self.db_path = str(self.storage_dir / "storage.db")
        self.init_database()

        # åˆå§‹åŒ–AIåŠŸèƒ½
        self.init_ai_models()

        # å¤©æ°”ç¼“å­˜
        self.latest_weather: Optional[Dict[str, Any]] = None
        # é¥æ„Ÿç¼“å­˜
        self.latest_remote_sensing: Optional[Dict[str, Any]] = None

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨ï¼ˆåŒé‡ä¿é™©ï¼‰
            db_path_obj = Path(self.db_path)
            db_path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # æ–‡ä»¶è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    file_size INTEGER,
                    file_type TEXT,
                    folder_id INTEGER,
                    upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    checksum TEXT,
                    is_cached BOOLEAN DEFAULT FALSE,
                    FOREIGN KEY (folder_id) REFERENCES folders (id)
                )
            ''')

            # æ–‡ä»¶å¤¹è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS folders (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    folder_name TEXT NOT NULL,
                    parent_folder_id INTEGER,
                    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (parent_folder_id) REFERENCES folders (id)
                )
            ''')

            # ä¸Šä¼ è¿›åº¦è¡¨ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS upload_progress (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT NOT NULL,
                    total_size INTEGER,
                    uploaded_size INTEGER,
                    chunk_size INTEGER,
                    checksum TEXT,
                    upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # AIåˆ†æç»“æœè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ai_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_id INTEGER,
                    analysis_type TEXT,
                    industry_category TEXT,
                    extracted_text TEXT,
                    key_phrases TEXT,
                    summary TEXT,
                    confidence_score REAL,
                    method TEXT,
                    analysis_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (file_id) REFERENCES files (id)
                )
            ''')

            # è¿ç§»ï¼šè‹¥æ—§è¡¨æ—  method åˆ—åˆ™è¡¥å……
            try:
                cursor.execute("PRAGMA table_info(ai_analysis)")
                cols = [row[1] for row in cursor.fetchall()]
                if 'method' not in cols:
                    cursor.execute('ALTER TABLE ai_analysis ADD COLUMN method TEXT')
                # æ·»åŠ ocr_contentå­—æ®µç”¨äºå­˜å‚¨å®Œæ•´çš„OCRå†…å®¹
                if 'ocr_content' not in cols:
                    cursor.execute('ALTER TABLE ai_analysis ADD COLUMN ocr_content TEXT')
            except Exception:
                pass

            # è¡Œä¸šåˆ†ç±»è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS industry_categories (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    category_name TEXT UNIQUE,
                    keywords TEXT,
                    description TEXT,
                    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            conn.commit()
            conn.close()
        except sqlite3.Error as e:
            import os
            error_msg = f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}\næ•°æ®åº“è·¯å¾„: {self.db_path}\nç›®å½•å­˜åœ¨: {os.path.exists(db_path_obj.parent)}"
            print(f"[ERROR] {error_msg}")
            raise RuntimeError(error_msg) from e
        except Exception as e:
            import os
            error_msg = f"æ•°æ®åº“åˆå§‹åŒ–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\næ•°æ®åº“è·¯å¾„: {self.db_path}"
            print(f"[ERROR] {error_msg}")
            raise RuntimeError(error_msg) from e

    def init_ai_models(self):
        """åˆå§‹åŒ–AIæ¨¡å‹"""
        # åˆå§‹åŒ–è¡Œä¸šåˆ†ç±»å…³é”®è¯ï¼ˆAgribusinessç»†åˆ†ï¼Œè¡¥å……éæ´²å¸¸è§ä½œç‰©/è¦ç´ ï¼‰
        self.industry_keywords = {
            "ç§æ¤ä¸š": ["ä½œç‰©", "ç‰ç±³", "å°ç±³", "é«˜ç²±", "æ°´ç¨»", "æœ¨è–¯", "å±±è¯", "çº¢è–¯", "èŠ±ç”Ÿ", "èŠéº»", "è‘µèŠ±ç±½", "æ£‰èŠ±",
                       "å¯å¯", "å’–å•¡", "èŒ¶å¶", "é¦™è•‰", "èŠ’æœ", "è è", "è”¬èœ", "æœå›­", "äº§é‡", "å•äº§", "å…¬é¡·", "äº©",
                       "æ’­ç§", "æ”¶è·", "çŒæº‰", "ç—…è™«å®³", "é™¤è‰", "å¯†åº¦"],
            "ç•œç‰§ä¸š": ["ç”ŸçŒª", "ç‰›ç¾Š", "å®¶ç¦½", "å¥¶ç‰›", "å‡ºæ ", "å­˜æ ", "é¥²æ–™", "æ—¥é¾„", "å¢é‡", "æ–™è‚‰æ¯”", "å…ç–«", "å…½è¯",
                       "ç–«ç—…", "ç¹è‚²", "çŠŠç‰›", "å± å®°"],
            "å†œèµ„ä¸åœŸå£¤": ["è‚¥æ–™", "æ°®è‚¥", "ç£·è‚¥", "é’¾è‚¥", "é…æ–¹æ–½è‚¥", "æœ‰æœºè´¨", "pH", "åœŸå£¤ç›åˆ†", "å¾®é‡å…ƒç´ ", "ä¿æ°´",
                           "è¦†ç›–", "æ·±æ¾", "ç§¸ç§†è¿˜ç”°"],
            "å†œä¸šé‡‘è": ["é‡‡è´­", "æˆæœ¬", "è´·æ¬¾", "ä¿å•", "ä¿é™©", "èµ”ä»˜", "ä¿è´¹", "æˆä¿¡", "ç°é‡‘æµ", "åº”æ”¶", "åº”ä»˜",
                         "åˆ©æ¶¦", "æ¯›åˆ©ç‡", "ä»·æ ¼", "æœŸè´§"],
            "ä¾›åº”é“¾ä¸ä»“å‚¨": ["å†·é“¾", "ä»“å‚¨", "ç‰©æµ", "è¿è¾“", "åº“å®¹", "æŸè€—", "å‘¨è½¬", "äº¤ä»˜", "è®¢å•", "æ‰¹æ¬¡", "è¿½æº¯"],
            "æ°”å€™ä¸é¥æ„Ÿ": ["é™é›¨", "é™æ°´", "æ¸©åº¦", "ç§¯æ¸©", "è’¸æ•£", "å¹²æ—±", "NDVI", "EVI", "å«æ˜Ÿ", "é¥æ„Ÿ", "æ°”è±¡ç«™",
                           "è¾å°„", "æ²™æ¼ è—è™«", "è‰åœ°è´ªå¤œè›¾"],
            "å†œä¸šç‰©è”ç½‘": ["ä¼ æ„Ÿå™¨", "æ¹¿åº¦", "å«æ°´ç‡", "EC", "é˜ˆå€¼", "é˜€é—¨", "æ³µç«™", "æ»´çŒ", "å–·çŒ", "è‡ªåŠ¨åŒ–", "æŠ¥è­¦"]
        }

        # åˆå§‹åŒ–DeepSeek APIå¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡æˆ–Streamlit secretsè·å–ï¼‰
        import os
        self.deepseek_api_key = ''
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
        self.deepseek_api_key = os.getenv('DEEPSEEK_API_KEY', '')
        # å¦‚æœç¯å¢ƒå˜é‡æ²¡æœ‰ï¼Œå°è¯•ä»Streamlit secretsè¯»å–
        if not self.deepseek_api_key:
            try:
                if hasattr(st, 'secrets') and st.secrets:
                    self.deepseek_api_key = st.secrets.get('DEEPSEEK_API_KEY', '')
            except Exception as e:
                # å¦‚æœè¯»å–secretså¤±è´¥ï¼Œè®°å½•ä½†ä¸ä¸­æ–­åˆå§‹åŒ–
                pass
        # æ¸…ç†å¯†é’¥ï¼ˆå»é™¤å¯èƒ½çš„ç©ºæ ¼ï¼‰
        if self.deepseek_api_key:
            self.deepseek_api_key = self.deepseek_api_key.strip()
        self.deepseek_api_url = "https://api.deepseek.com/v1/chat/completions"
        self.deepseek_model = "deepseek-chat"  # æˆ– "deepseek-coder" ç”¨äºä»£ç ç”Ÿæˆ
        # æ³¨æ„ï¼šå¦‚æœä½¿ç”¨DeepSeek-V3ï¼Œæ¨¡å‹åç§°åº”ä¸º "deepseek-chat"

        # åˆå§‹åŒ–OCR - ä»…ä½¿ç”¨Tesseract OCRï¼ˆè½»é‡çº§ï¼Œæ— éœ€åŠ è½½æ¨¡å‹ï¼‰
        self.ocr_available = TESSERACT_AVAILABLE
        self.ocr_load_failed = False
        
        if TESSERACT_AVAILABLE:
            print(f"[DEBUG] âœ… OCRåˆå§‹åŒ– - ä½¿ç”¨Tesseract OCRï¼ˆè½»é‡çº§ï¼Œå†…å­˜å ç”¨çº¦50-100MBï¼Œæ— éœ€åŠ è½½æ¨¡å‹ï¼‰")
        else:
            import platform
            system = platform.system()
            print(f"[DEBUG] âš ï¸ OCRåˆå§‹åŒ– - Tesseractä¸å¯ç”¨")
            if system == "Windows":
                print(f"[DEBUG] ğŸ’¡ Windowså®‰è£…è¯´æ˜:")
                print(f"[DEBUG]    1. ä¸‹è½½å®‰è£…: https://github.com/UB-Mannheim/tesseract/wiki")
                print(f"[DEBUG]    2. å®‰è£…æ—¶é€‰æ‹©ä¸­æ–‡è¯­è¨€åŒ…")
                print(f"[DEBUG]    3. æ·»åŠ åˆ°ç³»ç»ŸPATH")
                print(f"[DEBUG]    4. è¯¦ç»†è¯´æ˜: INSTALL_TESSERACT_WINDOWS.md")
            elif system == "Linux":
                print(f"[DEBUG] ğŸ’¡ Linuxå®‰è£…: sudo apt-get install tesseract-ocr tesseract-ocr-eng tesseract-ocr-chi-sim")
            elif system == "Darwin":  # macOS
                print(f"[DEBUG] ğŸ’¡ macOSå®‰è£…: brew install tesseract")
            print(f"[DEBUG] ğŸ’¡ Pythonä¾èµ–: pip install pytesseract Pillow")

        # åˆå§‹åŒ–æ–‡æœ¬åˆ†ç±»æ¨¡å‹
        self.text_classifier = None
        if TRANSFORMERS_AVAILABLE:
            try:
                # ä½¿ç”¨ä¸­æ–‡BERTæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»
                self.text_classifier = pipeline(
                    "text-classification",
                    model="bert-base-chinese",
                    tokenizer="bert-base-chinese"
                )
                st.success("âœ… BERT text classification model loaded successfully")
            except Exception as e:
                # Downgrade to info to avoid noisy toast; rules/ML will fallback
                pass  # é™é»˜å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
        # else:
        #     st.info("â„¹ï¸ Transformers library not installed, using machine learning classification")

        # åˆå§‹åŒ–æ‘˜è¦ç”Ÿæˆæ¨¡å‹
        self.summarizer = None
        if TRANSFORMERS_AVAILABLE:
            try:
                # ä½¿ç”¨T5æ¨¡å‹è¿›è¡Œæ‘˜è¦ç”Ÿæˆ
                self.summarizer = pipeline(
                    "summarization",
                    model="t5-small",
                    tokenizer="t5-small"
                )
                st.success("âœ… T5 summarization model loaded successfully")
            except Exception as e:
                # é™é»˜å¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½è§„åˆ™
                pass
        # else:
        #     st.info("â„¹ï¸ Using smart summarization algorithm")

        # åˆå§‹åŒ–æœºå™¨å­¦ä¹ åˆ†ç±»å™¨
        self.ml_classifier = None
        self.ml_trained = False
        if ML_AVAILABLE:
            try:
                # ä½¿ç”¨æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
                self.ml_classifier = Pipeline([
                    ('tfidf', TfidfVectorizer(max_features=1000, stop_words=None)),
                    ('classifier', MultinomialNB())
                ])
                # è‡ªåŠ¨åˆå§‹åŒ–é¢„è®­ç»ƒåˆ†ç±»å™¨
                if self.init_pretrained_classifier():
                    # æˆåŠŸæ—¶é™é»˜ï¼Œé¿å…è¿‡å¤šæç¤º
                    pass
                # else:
                #     st.info("Pre-trained ML classifier unavailable, using keyword matching")
            except Exception as e:
                # é™é»˜å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…
                pass
        # else:
        #     st.info("â„¹ï¸ ä½¿ç”¨å…³é”®è¯åŒ¹é…åˆ†ç±»")

        # åˆå§‹åŒ–é»˜è®¤è¡Œä¸šåˆ†ç±»
        self.init_default_categories()

    def fetch_weather_summary(self, latitude: float, longitude: float) -> Dict[str, Any]:
        """ä» Open-Meteo è·å–æœªæ¥7å¤©çš„æ°”è±¡æ‘˜è¦ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰"""
        try:
            url = (
                "https://api.open-meteo.com/v1/forecast"
                f"?latitude={latitude}&longitude={longitude}"
                "&hourly=temperature_2m,precipitation"
                "&daily=precipitation_sum,temperature_2m_max,temperature_2m_min"
                "&forecast_days=7&timezone=auto"
            )
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            daily = data.get("daily", {})
            result = {
                "location": {"lat": latitude, "lon": longitude},
                "precipitation_sum": daily.get("precipitation_sum", []),
                "tmax": daily.get("temperature_2m_max", []),
                "tmin": daily.get("temperature_2m_min", []),
                "dates": daily.get("time", [])
            }
            # ç®€è¦ç»Ÿè®¡
            try:
                total_rain = float(sum(x for x in result["precipitation_sum"] if isinstance(x, (int, float))))
            except Exception:
                total_rain = 0.0
            result["summary"] = {
                "7d_total_rain_mm": round(total_rain, 1),
                "avg_tmax": round(sum(result["tmax"]) / max(1, len(result["tmax"])), 1) if result["tmax"] else None,
                "avg_tmin": round(sum(result["tmin"]) / max(1, len(result["tmin"])), 1) if result["tmin"] else None,
            }
            self.latest_weather = result
            return {"success": True, "weather": result}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def compute_remote_sensing_stub(self, latitude: float, longitude: float, days: int = 30) -> Dict[str, Any]:
        """é¥æ„ŸæŒ‡æ•°å ä½ï¼šç”Ÿæˆè¿‘dayså¤©çš„NDVI/EVIç®€æ˜“æ—¶åºï¼ˆæ— éœ€å¤–éƒ¨æœåŠ¡ï¼‰ã€‚"""
        try:
            import math
            base_date = datetime.now()
            dates = [(base_date - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days - 1, -1, -1)]
            ndvi = []
            evi = []
            for i in range(days):
                # ç”Ÿæˆå¹³æ»‘çš„æ³¢åŠ¨æ•°æ®ï¼ŒèŒƒå›´åšç‰©ç†åˆç†çº¦æŸ
                v = 0.5 + 0.3 * math.sin(i / 6.0) + 0.1 * math.sin(i / 2.5)
                ndvi.append(round(max(0.0, min(0.9, v)), 3))
                e = 0.4 + 0.25 * math.sin(i / 7.0 + 0.5)
                evi.append(round(max(0.0, min(0.8, e)), 3))
            summary = {
                "ndvi_avg": round(sum(ndvi) / len(ndvi), 3) if ndvi else None,
                "evi_avg": round(sum(evi) / len(evi), 3) if evi else None,
                "ndvi_last": ndvi[-1] if ndvi else None,
                "evi_last": evi[-1] if evi else None,
            }
            result = {
                "location": {"lat": latitude, "lon": longitude},
                "dates": dates,
                "ndvi": ndvi,
                "evi": evi,
                "summary": summary,
            }
            self.latest_remote_sensing = result
            return {"success": True, "remote_sensing": result}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def extract_agri_structured_fields(self, text: str) -> Dict[str, Any]:
        """å†œä¸šæŠ¥è¡¨æ¨¡æ¿æŠ½å–ï¼ˆè§„åˆ™ç‰ˆå ä½ï¼‰ï¼šä½œç‰©ã€é¢ç§¯ã€æ—¥æœŸã€æ–½è‚¥/çŒæº‰/ç”¨è¯/å•äº§ç­‰ã€‚"""
        if not text:
            return {}
        import re
        fields: Dict[str, Any] = {}
        try:
            # ä½œç‰©
            m = re.search(r'(ä½œç‰©|å“ç§|ä½œç‰©åç§°)[ï¼š:ï¼Œ]\s*([\u4e00-\u9fffA-Za-z0-9]+)', text)
            if m: fields['ä½œç‰©'] = m.group(2)
            # é¢ç§¯ï¼ˆäº©/å…¬é¡·/haï¼‰
            m = re.search(r'(é¢ç§¯|æ’­ç§é¢ç§¯|æ”¶è·é¢ç§¯)[ï¼š:ï¼Œ]\s*([\d,.]+)\s*(äº©|å…¬é¡·|ha)', text)
            if m: fields['é¢ç§¯'] = f"{m.group(2)} {m.group(3)}"
            # æ—¥æœŸï¼ˆç®€å•è¯†åˆ« å¹´-æœˆ-æ—¥ æˆ– å¹´/æœˆ/æ—¥ æˆ– ä¸­æ–‡ï¼‰
            m = re.search(r'(æ—¥æœŸ|æ—¶é—´|è®°å½•æ—¶é—´)[ï¼š:ï¼Œ]\s*(\d{4}[-å¹´/]\d{1,2}[-æœˆ/]\d{1,2})', text)
            if m: fields['æ—¥æœŸ'] = m.group(2)
            # æ–½è‚¥
            m = re.search(r'(æ–½è‚¥|è‚¥æ–™|é…æ–¹æ–½è‚¥)[ï¼š:ï¼Œ]?\s*([\u4e00-\u9fffA-Za-z0-9]+)?\s*([\d,.]+)\s*(kg|å…¬æ–¤|æ–¤)', text)
            if m: fields['æ–½è‚¥'] = f"{(m.group(2) or '').strip()} {m.group(3)} {m.group(4)}".strip()
            # çŒæº‰
            m = re.search(r'(çŒæº‰|æµ‡æ°´)[ï¼š:ï¼Œ]?\s*([\d,.]+)\s*(mm|ç«‹æ–¹|m3|æ–¹)', text)
            if m: fields['çŒæº‰'] = f"{m.group(2)} {m.group(3)}"
            # ç”¨è¯
            m = re.search(r'(å†œè¯|ç”¨è¯|é˜²æ²»)[ï¼š:ï¼Œ]?\s*([\u4e00-\u9fffA-Za-z0-9]+)\s*([\d,.]+)\s*(ml|æ¯«å‡|L|å‡|kg|å…‹|g)',
                          text)
            if m: fields['ç”¨è¯'] = f"{m.group(2)} {m.group(3)} {m.group(4)}"
            # å•äº§/äº§é‡
            m = re.search(r'(å•äº§|äº©äº§)[ï¼š:ï¼Œ]\s*([\d,.]+)\s*(æ–¤/äº©|å…¬æ–¤/äº©|kg/ha|t/ha)', text)
            if m: fields['å•äº§'] = f"{m.group(2)} {m.group(3)}"
            m = re.search(r'(æ€»äº§|äº§é‡)[ï¼š:ï¼Œ]\s*([\d,.]+)\s*(kg|å¨|t)', text)
            if m: fields['äº§é‡'] = f"{m.group(2)} {m.group(3)}"
        except Exception:
            pass
        return fields

    def init_default_categories(self):
        """åˆå§‹åŒ–é»˜è®¤è¡Œä¸šåˆ†ç±»"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        for category, keywords in self.industry_keywords.items():
            cursor.execute('''
                INSERT OR IGNORE INTO industry_categories (category_name, keywords, description)
                VALUES (?, ?, ?)
            ''', (category, json.dumps(keywords, ensure_ascii=False), f"{category}ç›¸å…³æ–‡æ¡£"))

        conn.commit()
        conn.close()

    def _to_english_category(self, category: str) -> str:
        """å°†åˆ†ç±»åç§°è½¬æ¢ä¸ºè‹±æ–‡ï¼ˆç»Ÿä¸€å­˜å‚¨æ ¼å¼ï¼‰"""
        mapping = {
            "ç§æ¤ä¸š": "Planting",
            "ç•œç‰§ä¸š": "Livestock",
            "å†œèµ„ä¸åœŸå£¤": "Inputs-Soil",
            "å†œä¸šé‡‘è": "Agri-Finance",
            "ä¾›åº”é“¾ä¸ä»“å‚¨": "SupplyChain-Storage",
            "æ°”å€™ä¸é¥æ„Ÿ": "Climate-RemoteSensing",
            "å†œä¸šç‰©è”ç½‘": "Agri-IoT",
            "æœªåˆ†ç±»": "Unclassified",  # æ·»åŠ æœªåˆ†ç±»çš„æ˜ å°„
        }
        # å¦‚æœä¸åœ¨æ˜ å°„è¡¨ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯è‹±æ–‡åˆ†ç±»åç§°
        if category in mapping:
            return mapping[category]
        # å¦‚æœå·²ç»æ˜¯è‹±æ–‡åˆ†ç±»åç§°ï¼Œç›´æ¥è¿”å›
        english_categories = ["Planting", "Livestock", "Inputs-Soil", "Agri-Finance", 
                            "SupplyChain-Storage", "Climate-RemoteSensing", "Agri-IoT", "Unclassified"]
        if category in english_categories:
            return category
        # å¦åˆ™è¿”å›Unclassifiedä½œä¸ºé»˜è®¤å€¼
        return "Unclassified"
    
    def _extract_classification_from_ai_response(self, ai_response: str, extracted_text: str) -> Optional[Dict[str, Any]]:
        """ä»DeepSeek AIå“åº”ä¸­æå–è¡Œä¸šåˆ†ç±»ï¼Œåªè¿”å›ä¸å·¥ä¸šè§†å›¾åŒ¹é…çš„æ ‡ç­¾"""
        try:
            import re
            
            # å®šä¹‰æœ‰æ•ˆçš„åˆ†ç±»æ ‡ç­¾ï¼ˆä¸å·¥ä¸šè§†å›¾ä¸€è‡´ï¼‰
            valid_categories = ["Planting", "Livestock", "Inputs-Soil", "Agri-Finance", 
                              "SupplyChain-Storage", "Climate-RemoteSensing", "Agri-IoT", "Unclassified"]
            
            # é¦–å…ˆå°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æå–æ˜ç¡®çš„åˆ†ç±»å£°æ˜
            pattern = r'(?:Industry\s*(?:Classification|Category)?\s*:?\s*|åˆ†ç±»\s*:?\s*)([A-Za-z-]+)'
            match = re.search(pattern, ai_response, re.IGNORECASE)
            if match:
                category_name = match.group(1).strip()
                # æ ‡å‡†åŒ–åˆ†ç±»åç§°
                category_mapping = {
                    "planting": "Planting",
                    "livestock": "Livestock",
                    "inputs-soil": "Inputs-Soil",
                    "inputsoil": "Inputs-Soil",
                    "agri-finance": "Agri-Finance",
                    "agrifinance": "Agri-Finance",
                    "supplychain-storage": "SupplyChain-Storage",
                    "supplychainstorage": "SupplyChain-Storage",
                    "climate-remotesensing": "Climate-RemoteSensing",
                    "climateremotesensing": "Climate-RemoteSensing",
                    "agri-iot": "Agri-IoT",
                    "agriiot": "Agri-IoT",
                    "unclassified": "Unclassified"
                }
                normalized_category = category_mapping.get(category_name.lower())
                if normalized_category and normalized_category in valid_categories:
                    print(f"[DEBUG] _extract_classification_from_ai_response: ä»AIå“åº”ä¸­ç›´æ¥æå–åˆ†ç±»: {normalized_category}")
                    return {
                        "category": normalized_category,
                        "confidence": 0.8,
                        "method": "AI Response Direct Extraction"
                    }
            
            # å¦‚æœæ— æ³•ç›´æ¥æå–ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼ˆä½¿ç”¨ä¸classify_industryç›¸åŒçš„å…³é”®è¯ï¼‰
            combined_text = (ai_response + " " + extracted_text).lower()
            category_scores = {}
            
            for category, keywords in self.industry_keywords.items():
                score = 0
                for keyword in keywords:
                    count = combined_text.count(keyword.lower())
                    if count > 0:
                        score += count
                category_scores[category] = score
            
            # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
            if category_scores and max(category_scores.values()) > 0:
                best_category = max(category_scores, key=category_scores.get)
                max_score = category_scores[best_category]
                
                # è®¡ç®—ç½®ä¿¡åº¦
                total_keywords = len(self.industry_keywords[best_category])
                confidence = min(max_score / (total_keywords * 1.5), 1.0)
                
                # å¦‚æœç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼Œç›´æ¥è¿”å›Unclassified
                if confidence < 0.1:
                    print(f"[DEBUG] _extract_classification_from_ai_response: ç½®ä¿¡åº¦å¤ªä½ ({confidence:.2f})ï¼Œè¿”å›Unclassified")
                    return {"category": "Unclassified", "confidence": 0.0, "method": "AI Response (Low Confidence)"}
                
                # è½¬æ¢ä¸ºè‹±æ–‡åˆ†ç±»åç§°
                eng_category = self._to_english_category(best_category)
                
                print(f"[DEBUG] _extract_classification_from_ai_response: ä»AIå“åº”ä¸­å…³é”®è¯åŒ¹é…åˆ†ç±»: {eng_category}, ç½®ä¿¡åº¦: {confidence:.2f}")
                return {
                    "category": eng_category,
                    "confidence": confidence,
                    "method": "AI Response Keyword Matching"
                }
            else:
                print(f"[DEBUG] _extract_classification_from_ai_response: æ— æ³•ä»AIå“åº”ä¸­æå–åˆ†ç±»ï¼Œè¿”å›Unclassified")
                return {"category": "Unclassified", "confidence": 0.0, "method": "AI Response (No Match)"}
                
        except Exception as e:
            print(f"[DEBUG] _extract_classification_from_ai_response: é”™è¯¯: {str(e)}")
            return {"category": "Unclassified", "confidence": 0.0, "method": "AI Response (Error)"}

    def generate_smart_report(self, file_id: int) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½æŠ¥å‘Šå’Œå›¾è¡¨"""
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT file_path, file_type, filename FROM files WHERE id = ?', (file_id,))
            result = cursor.fetchone()
            conn.close()

            if not result:
                return {"success": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}

            file_path, file_type, filename = result

            # æå–æ–‡æœ¬å†…å®¹
            text = self.extract_text_from_file(file_id)
            if not text:
                return {"success": False, "error": "æ— æ³•æå–æ–‡æœ¬å†…å®¹"}

            # åˆ†ææ–‡æ¡£ç»“æ„
            analysis = self.analyze_document_structure(text)
            analysis["full_text"] = text

            # æå–æ•°æ®ç‚¹
            data_points = self.extract_data_points(text)

            # ç”Ÿæˆå›¾è¡¨
            charts = self.generate_charts(data_points)

            # ç”ŸæˆæŠ¥å‘Š
            report = self.create_smart_report(analysis, charts, filename)

            return {
                "success": True,
                "analysis": analysis,
                "data_points": data_points,
                "charts": charts,
                "report": report
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def call_deepseek_api(self, messages: List[Dict[str, str]], max_tokens: int = 2000, temperature: float = 0.7) -> Optional[str]:
        """è°ƒç”¨DeepSeek APIè¿›è¡Œå¯¹è¯"""
        if not self.deepseek_api_key:
            return None
        
        # æ¸…ç†APIå¯†é’¥ï¼ˆå»é™¤å¯èƒ½çš„ç©ºæ ¼å’Œæ¢è¡Œç¬¦ï¼‰
        api_key = self.deepseek_api_key.strip()
        if not api_key:
            st.error("DeepSeek API key is empty, please check configuration")
            return None
        
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            
            data = {
                "model": self.deepseek_model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": False
            }
            
            # è°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨å¼€å‘æ—¶æ˜¾ç¤ºï¼‰
            if st.session_state.get('debug_mode', False):
                st.write(f"API URL: {self.deepseek_api_url}")
                st.write(f"Model: {self.deepseek_model}")
                st.write(f"API Key (å‰10ä½): {api_key[:10]}...")
            
            response = requests.post(
                self.deepseek_api_url,
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    choice = result['choices'][0]
                    content = choice['message']['content']
                    
                    # æ£€æŸ¥å“åº”æ˜¯å¦å®Œæ•´
                    finish_reason = choice.get('finish_reason', '')
                    if finish_reason == 'length':
                        # å“åº”å› è¾¾åˆ°max_tokensé™åˆ¶è€Œè¢«æˆªæ–­
                        st.warning("âš ï¸ AI response was truncated due to token limit. Consider increasing max_tokens or asking a more specific question.")
                        # ä»ç„¶è¿”å›å†…å®¹ï¼Œä½†æ·»åŠ æç¤º
                        return content + "\n\n[Note: Response may be incomplete due to token limit]"
                    elif finish_reason == 'stop':
                        # æ­£å¸¸å®Œæˆ
                        return content
                    else:
                        # å…¶ä»–æƒ…å†µï¼Œä»ç„¶è¿”å›å†…å®¹
                        return content
                else:
                    st.warning(f"API response format abnormal: {result}")
                    return None
            elif response.status_code == 401:
                error_msg = "DeepSeek API authentication failed"
                try:
                    error_data = response.json()
                    if 'error' in error_data:
                        error_msg = f"Authentication failed: {error_data['error'].get('message', 'Invalid API key')}"
                except:
                    error_msg = f"Authentication failed: {response.text}"
                st.error(error_msg)
                st.info("ğŸ’¡ Please check:\n1. Is the API key correct (in .secrets.toml)?\n2. Is the API key valid and not expired?\n3. Is the key format correct (should start with sk-)?")
                return None
            else:
                error_msg = f"DeepSeek API error: {response.status_code}"
                try:
                    error_data = response.json()
                    if 'error' in error_data:
                        error_msg = f"{error_msg} - {error_data['error'].get('message', response.text)}"
                except:
                    error_msg = f"{error_msg} - {response.text}"
                st.error(error_msg)
                return None
                
        except requests.exceptions.Timeout:
            st.error("DeepSeek API request timeout, please check network connection")
            return None
        except requests.exceptions.ConnectionError:
            st.error("Unable to connect to DeepSeek API, please check network connection")
            return None
        except Exception as e:
            st.error(f"Failed to call DeepSeek API: {str(e)}")
            return None

    def generate_ai_report(self, file_id: int, user_question) -> Dict[str, Any]:
        """ä½¿ç”¨DeepSeek AIç”Ÿæˆæ™ºèƒ½æŠ¥å‘Šå’Œå›ç­”ç”¨æˆ·é—®é¢˜
        
        å¤„ç†é€»è¾‘ï¼š
        1. æ–‡æ¡£ç±»ï¼ˆ.txt, .docxç­‰ï¼‰ï¼šç›´æ¥è¯»å–æ–‡æ¡£å†…å®¹ï¼Œç»“åˆç”¨æˆ·é—®é¢˜æé—®
        2. å›¾ç‰‡æˆ–PDFï¼šå…ˆç”¨Tesseract OCRè¿›è¡Œæå–ï¼Œç„¶åç»“åˆç”¨æˆ·é—®é¢˜å‘ç»™deepseek
        3. Excelæˆ–xlsxï¼šä¿ç•™åŸæ¥çš„åˆ†æç¨‹åº
        """
        try:
            if not self.deepseek_api_key:
                return {"success": False, "error": "æœªé…ç½®DeepSeek APIå¯†é’¥ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–Streamlit secretsä¸­è®¾ç½®DEEPSEEK_API_KEY"}
            
            start_time = time.time()
            # è·å–æ–‡ä»¶ä¿¡æ¯
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT file_path, file_type, filename FROM files WHERE id = ?', (file_id,))
            result = cursor.fetchone()
            conn.close()

            if not result:
                return {"success": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}

            file_path, file_type, filename = result
            print(f"[DEBUG] generate_ai_report: å¼€å§‹å¤„ç† - file_id: {file_id}, file_type: {file_type}, filename: {filename}")

            # æå–æ–‡ä»¶å†…å®¹
            file_content = ""
            df = None
            
            # ========== é€»è¾‘1: Excel/CSVæ–‡ä»¶ - ä¿ç•™åŸæ¥çš„åˆ†æç¨‹åº ==========
            if filename.endswith(('.xlsx', '.xls', '.csv')):
                print(f"[DEBUG] generate_ai_report: æ£€æµ‹åˆ°Excel/CSVæ–‡ä»¶ï¼Œä½¿ç”¨åŸæœ‰åˆ†æç¨‹åº")
                df = self.extract_excel_csv(file_id)
                if df is not None:
                    # å°†DataFrameè½¬æ¢ä¸ºæ–‡æœ¬æè¿°
                    file_content = f"æ–‡ä»¶ç±»å‹: Excel/CSV\n"
                    file_content += f"æ–‡ä»¶å: {filename}\n"
                    file_content += f"æ•°æ®å½¢çŠ¶: {df.shape[0]}è¡Œ x {df.shape[1]}åˆ—\n"
                    file_content += f"åˆ—å: {', '.join(df.columns.tolist())}\n\n"
                    file_content += f"æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰:\n{df.head(10).to_string()}\n\n"
                    file_content += f"æ•°æ®ç»Ÿè®¡ä¿¡æ¯:\n{df.describe().to_string()}\n\n"
                    # å¦‚æœæ•°æ®é‡ä¸å¤§ï¼ŒåŒ…å«å®Œæ•´æ•°æ®
                    if len(df) <= 100:
                        file_content += f"å®Œæ•´æ•°æ®:\n{df.to_string()}\n"
                    print(f"[DEBUG] generate_ai_report: Excel/CSVæ•°æ®æå–å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(file_content)}")
                else:
                    return {"success": False, "error": "æ— æ³•è¯»å–Excel/CSVæ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ ¼å¼æ­£ç¡®"}
            
            # ========== é€»è¾‘2: å›¾ç‰‡æˆ–PDFæ–‡ä»¶ - ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„OCRå†…å®¹ ==========
            elif file_type == 'image' or (file_type == 'application' and filename.endswith('.pdf')):
                print(f"[DEBUG] generate_ai_report: æ£€æµ‹åˆ°å›¾ç‰‡æˆ–PDFæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„OCRå†…å®¹")
                
                # å…ˆå°è¯•ä»æ•°æ®åº“è¯»å–OCRå†…å®¹
                conn_ocr = sqlite3.connect(self.db_path)
                cursor_ocr = conn_ocr.cursor()
                cursor_ocr.execute('''
                    SELECT ocr_content FROM ai_analysis 
                    WHERE file_id = ? AND ocr_content IS NOT NULL AND ocr_content != ''
                    ORDER BY analysis_time DESC LIMIT 1
                ''', (file_id,))
                ocr_result = cursor_ocr.fetchone()
                conn_ocr.close()
                
                if ocr_result and ocr_result[0]:
                    # ä½¿ç”¨æ•°æ®åº“ä¸­çš„OCRå†…å®¹
                    ocr_text = ocr_result[0]
                    print(f"[DEBUG] generate_ai_report: âœ… ä»æ•°æ®åº“è¯»å–OCRå†…å®¹ï¼Œé•¿åº¦: {len(ocr_text)}")
                    file_content = f"File Type: {'Image' if file_type == 'image' else 'PDF'}\n"
                    file_content += f"Filename: {filename}\n\n"
                    file_content += f"OCR Recognized Text:\n{ocr_text}"
                    st.info("âœ… ä½¿ç”¨å·²ä¿å­˜çš„OCRå†…å®¹ï¼ˆæ— éœ€é‡æ–°è¯†åˆ«ï¼‰")
                elif OCR_AVAILABLE and TESSERACT_AVAILABLE:
                    # æ•°æ®åº“ä¸­æ²¡æœ‰OCRå†…å®¹ï¼Œæ‰§è¡ŒOCRæå–
                    print(f"[DEBUG] generate_ai_report: æ•°æ®åº“ä¸­æ²¡æœ‰OCRå†…å®¹ï¼Œå¼€å§‹æ‰§è¡ŒOCRæå–")
                
                    try:
                        # å»¶è¿ŸåŠ è½½OCRæ¨¡å‹
                        if not self._load_ocr_model():
                            print(f"[DEBUG] generate_ai_report: OCRæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡OCRæå–")
                            file_content = f"File Type: {'Image' if file_type == 'image' else 'PDF'}\n"
                            file_content += f"Filename: {filename}\n"
                            file_content += f"Note: OCR model loading failed, unable to extract text from file."
                            st.warning("âš ï¸ OCR model loading failed, skipping OCR extraction")
                        else:
                            # å¯¹äºPDFæ–‡ä»¶ï¼Œéœ€è¦å…ˆè½¬æ¢ä¸ºå›¾ç‰‡
                            ocr_file_path = file_path
                            is_pdf = filename.endswith('.pdf')
                            temp_images = []
                            
                            if is_pdf and PDF_AVAILABLE and fitz is not None:
                                print(f"[DEBUG] generate_ai_report: PDFæ–‡ä»¶ï¼Œå…ˆè½¬æ¢ä¸ºå›¾ç‰‡...")
                                try:
                                    doc = fitz.open(file_path)
                                    all_ocr_text = []
                                    
                                    # é™åˆ¶PDFé¡µæ•°ï¼Œé¿å…å†…å­˜æº¢å‡º
                                    max_pages = min(len(doc), 10)  # æœ€å¤šå¤„ç†10é¡µ
                                    if len(doc) > max_pages:
                                        print(f"[DEBUG] generate_ai_report: PDFæœ‰{len(doc)}é¡µï¼Œåªå¤„ç†å‰{max_pages}é¡µä»¥èŠ‚çœå†…å­˜")
                                        st.info(f"ğŸ“„ PDF has {len(doc)} pages, processing first {max_pages} pages to save memory")
                                    
                                    with st.spinner("ğŸ” Converting PDF to images and recognizing text..."):
                                        for page_num in range(max_pages):
                                            try:
                                                page = doc[page_num]
                                                # é™ä½ç¼©æ”¾æ¯”ä¾‹ä»¥èŠ‚çœå†…å­˜ï¼ˆä»2å€é™åˆ°1.5å€ï¼‰
                                                pix = page.get_pixmap(matrix=fitz.Matrix(1.5, 1.5))
                                                img_data = pix.tobytes("png")
                                                
                                                # æ£€æŸ¥å›¾ç‰‡å¤§å°ï¼Œå¦‚æœå¤ªå¤§åˆ™è·³è¿‡
                                                img_size_mb = len(img_data) / (1024 * 1024)
                                                if img_size_mb > 10:  # å¦‚æœå•é¡µå›¾ç‰‡è¶…è¿‡10MBï¼Œè·³è¿‡
                                                    print(f"[DEBUG] generate_ai_report: PDFç¬¬{page_num + 1}é¡µå›¾ç‰‡è¿‡å¤§({img_size_mb:.2f}MB)ï¼Œè·³è¿‡")
                                                    continue
                                                
                                                # ä¿å­˜ä¸´æ—¶å›¾ç‰‡
                                                import tempfile
                                                import os
                                                temp_img = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
                                                temp_img.write(img_data)
                                                temp_img.close()
                                                temp_images.append(temp_img.name)
                                                
                                                # å¯¹æ¯é¡µè¿›è¡ŒOCR
                                                print(f"[DEBUG] generate_ai_report: å¤„ç†PDFç¬¬ {page_num + 1} é¡µ...")
                                                try:
                                                    page_results = self._ocr_readtext(temp_img.name)
                                                    
                                                    if page_results and len(page_results) > 0:
                                                        page_text = ' '.join([result[1] for result in page_results])
                                                        all_ocr_text.append(f"Page {page_num + 1}:\n{page_text}")
                                                except MemoryError as e:
                                                    print(f"[DEBUG] generate_ai_report: PDFç¬¬{page_num + 1}é¡µOCRå†…å­˜ä¸è¶³: {str(e)}")
                                                    st.warning(f"âš ï¸ Page {page_num + 1} OCR failed due to insufficient memory")
                                                    break  # å†…å­˜ä¸è¶³æ—¶åœæ­¢å¤„ç†
                                                except Exception as e:
                                                    print(f"[DEBUG] generate_ai_report: PDFç¬¬{page_num + 1}é¡µOCRå¤±è´¥: {str(e)}")
                                                    # ç»§ç»­å¤„ç†ä¸‹ä¸€é¡µ
                                                    continue
                                            except MemoryError as e:
                                                print(f"[DEBUG] generate_ai_report: PDFç¬¬{page_num + 1}é¡µå¤„ç†å†…å­˜ä¸è¶³: {str(e)}")
                                                st.warning(f"âš ï¸ Page {page_num + 1} processing failed due to insufficient memory")
                                                break
                                            except Exception as e:
                                                print(f"[DEBUG] generate_ai_report: PDFç¬¬{page_num + 1}é¡µå¤„ç†å¤±è´¥: {str(e)}")
                                                continue
                                    
                                    doc.close()
                                    
                                    # æ¸…ç†ä¸´æ—¶å›¾ç‰‡
                                    for temp_img_path in temp_images:
                                        try:
                                            os.unlink(temp_img_path)
                                        except:
                                            pass
                                    
                                    if all_ocr_text:
                                        ocr_text = '\n\n'.join(all_ocr_text)
                                        print(f"[DEBUG] generate_ai_report: âœ… PDF OCRè¯†åˆ«æˆåŠŸï¼Œå…± {len(all_ocr_text)} é¡µï¼Œæ–‡å­—é•¿åº¦: {len(ocr_text)}")
                                        file_content = f"File Type: PDF\n"
                                        file_content += f"Filename: {filename}\n\n"
                                        file_content += f"OCR Recognized Text:\n{ocr_text}"
                                        st.success(f"âœ… PDF OCR recognition successful, recognized {len(all_ocr_text)} pages")
                                    else:
                                        print(f"[DEBUG] generate_ai_report: âš ï¸ PDF OCRæœªè¯†åˆ«åˆ°æ–‡å­—")
                                        file_content = f"File Type: PDF\n"
                                        file_content += f"Filename: {filename}\n"
                                        file_content += f"Note: No text content recognized in PDF, may be a scanned PDF or unclear text."
                                        st.warning("âš ï¸ PDF OCR did not recognize any text content")
                                        
                                except Exception as pdf_error:
                                    print(f"[DEBUG] generate_ai_report: PDFå¤„ç†å¤±è´¥: {str(pdf_error)}")
                                    # æ¸…ç†ä¸´æ—¶å›¾ç‰‡
                                    for temp_img_path in temp_images:
                                        try:
                                            import os
                                            os.unlink(temp_img_path)
                                        except:
                                            pass
                                    raise pdf_error
                            else:
                                # å›¾ç‰‡æ–‡ä»¶ç›´æ¥OCR
                                print(f"[DEBUG] generate_ai_report: å¼€å§‹OCRè¯†åˆ«å›¾ç‰‡: {file_path}")
                                
                                # æ£€æŸ¥å›¾ç‰‡å¤§å°å’Œå°ºå¯¸ï¼Œå¦‚æœå¤ªå¤§åˆ™ç¼©æ”¾
                                try:
                                    from PIL import Image
                                    img = Image.open(file_path)
                                    img_width, img_height = img.size
                                    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
                                    
                                    print(f"[DEBUG] generate_ai_report: å›¾ç‰‡å°ºå¯¸: {img_width}x{img_height}, æ–‡ä»¶å¤§å°: {file_size_mb:.2f}MB")
                                    
                                    # å¦‚æœå›¾ç‰‡å¤ªå¤§ï¼Œè¿›è¡Œç¼©æ”¾
                                    max_dimension = 2000  # æœ€å¤§å°ºå¯¸2000åƒç´ 
                                    max_file_size_mb = 5  # æœ€å¤§æ–‡ä»¶å¤§å°5MB
                                    
                                    if img_width > max_dimension or img_height > max_dimension or file_size_mb > max_file_size_mb:
                                        print(f"[DEBUG] generate_ai_report: å›¾ç‰‡è¿‡å¤§ï¼Œè¿›è¡Œç¼©æ”¾...")
                                        st.info(f"ğŸ“· Image is large ({img_width}x{img_height}, {file_size_mb:.1f}MB), resizing for OCR...")
                                        
                                        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                                        scale = min(max_dimension / img_width, max_dimension / img_height)
                                        new_width = int(img_width * scale)
                                        new_height = int(img_height * scale)
                                        
                                        # ç¼©æ”¾å›¾ç‰‡
                                        img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                                        
                                        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                                        import tempfile
                                        temp_img_path = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
                                        temp_img_path.close()
                                        img_resized.save(temp_img_path.name, 'PNG')
                                        
                                        ocr_file_path = temp_img_path.name
                                        temp_images.append(ocr_file_path)
                                        print(f"[DEBUG] generate_ai_report: å›¾ç‰‡å·²ç¼©æ”¾è‡³: {new_width}x{new_height}")
                                    else:
                                        ocr_file_path = file_path
                                except Exception as e:
                                    print(f"[DEBUG] generate_ai_report: å›¾ç‰‡æ£€æŸ¥å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶")
                                    ocr_file_path = file_path
                                
                                try:
                                    with st.spinner("ğŸ” Recognizing text in image..."):
                                        results = self._ocr_readtext(ocr_file_path)
                                    print(f"[DEBUG] generate_ai_report: OCRè¯†åˆ«å®Œæˆï¼Œç»“æœæ•°é‡: {len(results) if results else 0}")
                                except MemoryError as e:
                                    print(f"[DEBUG] generate_ai_report: OCRè¯†åˆ«å†…å­˜ä¸è¶³: {str(e)}")
                                    st.error("âŒ OCR recognition failed: Insufficient memory. The image may be too large.")
                                    file_content = f"File Type: Image\n"
                                    file_content += f"Filename: {filename}\n"
                                    file_content += f"Note: OCR recognition failed due to insufficient memory. Please try with a smaller image or disable OCR."
                                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿å¤–å±‚å¤„ç†
                                except Exception as e:
                                    print(f"[DEBUG] generate_ai_report: OCRè¯†åˆ«å¤±è´¥: {str(e)}")
                                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿å¤–å±‚å¤„ç†
                                
                                if results and len(results) > 0:
                                    ocr_text = ' '.join([result[1] for result in results])
                                    print(f"[DEBUG] generate_ai_report: âœ… OCRè¯†åˆ«æˆåŠŸï¼Œæ–‡å­—é•¿åº¦: {len(ocr_text)}")
                                    print(f"[DEBUG] generate_ai_report: OCRæ–‡å­—é¢„è§ˆ: {ocr_text[:200]}...")
                                    
                                    file_content = f"File Type: Image\n"
                                    file_content += f"Filename: {filename}\n\n"
                                    file_content += f"OCR Recognized Text:\n{ocr_text}"
                                    
                                    st.success(f"âœ… OCR recognition successful, recognized {len(results)} text regions")
                                else:
                                    print(f"[DEBUG] generate_ai_report: âš ï¸ OCRæœªè¯†åˆ«åˆ°æ–‡å­—")
                                    file_content = f"File Type: Image\n"
                                    file_content += f"Filename: {filename}\n"
                                    file_content += f"Note: No text content recognized in image, may be a pure image or unclear text."
                                    st.warning("âš ï¸ OCR did not recognize any text content")
                            
                    except Exception as e:
                        print(f"[DEBUG] generate_ai_report: âŒ OCRè¯†åˆ«å¤±è´¥: {str(e)}")
                        import traceback
                        print(f"[DEBUG] generate_ai_report: é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                        file_content = f"File Type: {'Image' if file_type == 'image' else 'PDF'}\n"
                        file_content += f"Filename: {filename}\n"
                        file_content += f"Note: OCR recognition failed ({str(e)}), unable to extract text from file."
                        st.error(f"âŒ OCR recognition failed: {str(e)}")
                else:
                    print(f"[DEBUG] generate_ai_report: OCRä¸å¯ç”¨")
                    file_content = f"File Type: {'Image' if file_type == 'image' else 'PDF'}\n"
                    file_content += f"Filename: {filename}\n"
                    file_content += f"Note: OCR feature unavailable, unable to recognize text in file. Please install Tesseract OCR. See INSTALL_TESSERACT.md for details."
                    st.warning("âš ï¸ OCR feature unavailable. Please install Tesseract OCR. See INSTALL_TESSERACT.md for details.")
            
            # ========== é€»è¾‘3: æ–‡æ¡£ç±»æ–‡ä»¶ - ç›´æ¥è¯»å–æ–‡æ¡£å†…å®¹ ==========
            else:
                print(f"[DEBUG] generate_ai_report: æ£€æµ‹åˆ°æ–‡æ¡£ç±»æ–‡ä»¶ï¼Œç›´æ¥è¯»å–å†…å®¹")
                file_content = self.extract_text_from_file(file_id)
                print(f"[DEBUG] generate_ai_report: æ–‡æ¡£å†…å®¹æå–å®Œæˆï¼Œé•¿åº¦: {len(file_content) if file_content else 0}")
                
                if not file_content or file_content.startswith("(No extractable text"):
                    print(f"[DEBUG] generate_ai_report: âš ï¸ æ— æ³•æå–æ–‡æ¡£å†…å®¹")
                    file_content = f"æ–‡ä»¶ç±»å‹: {file_type}\n"
                    file_content += f"æ–‡ä»¶å: {filename}\n"
                    file_content += f"æ³¨æ„: æ— æ³•ç›´æ¥æå–æ­¤æ–‡ä»¶ç±»å‹çš„æ–‡æœ¬å†…å®¹ï¼Œè¯·å°è¯•é¢„è§ˆæˆ–ä¸‹è½½æ–‡ä»¶æŸ¥çœ‹ã€‚\n"
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            if not file_content:
                print(f"[DEBUG] generate_ai_report: âŒ æœ€ç»ˆæ— æ³•è·å–æ–‡ä»¶å†…å®¹")
                return {"success": False, "error": "æ— æ³•æå–æ–‡ä»¶å†…å®¹ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ ¼å¼æ”¯æŒ"}
            
            print(f"[DEBUG] generate_ai_report: âœ… æ–‡ä»¶å†…å®¹æå–å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(file_content)}")

            # æ„å»ºæç¤ºè¯
            system_prompt = """You are a professional data analysis assistant. Please answer the user's questions based on the content of the uploaded file.
If the file is Excel or CSV data, please provide detailed data analysis, statistical information, and insights.
If the file is a document, please answer questions based on the document content.
Please answer in English, be accurate, detailed, and well-organized."""

            # é™åˆ¶å†…å®¹é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶
            file_content_limited = file_content[:8000]
            user_prompt = f"""File content:
{file_content_limited}

User question: {user_question}

Please answer the user's question based on the above file content."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            # è°ƒç”¨DeepSeek API - å¢åŠ max_tokensä»¥ç¡®ä¿å®Œæ•´å“åº”
            with st.spinner("ğŸ¤” DeepSeek AI is analyzing the file and generating response..."):
                ai_response = self.call_deepseek_api(messages, max_tokens=4000, temperature=0.7)
            
            generation_time = time.time() - start_time

            if ai_response:
                st.success(f"âœ… AI analysis completed, took {generation_time:.2f} seconds")
                
                # Display AI response
                st.markdown("#### ğŸ¤– AI Analysis Results")
                st.markdown(ai_response)
                
                # å¦‚æœæ˜¯æ•°æ®æ–‡ä»¶ï¼Œè¿˜å¯ä»¥ç”Ÿæˆå¯è§†åŒ–
                if df is not None and len(df) > 0:
                    with st.expander("ğŸ“Š Data Visualization (Optional)"):
                        try:
                            plt.close('all')
                            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
                            
                            # æ•°å€¼åˆ—çš„åˆ†å¸ƒ
                            numeric_cols = df.select_dtypes(include=[np.number]).columns
                            if len(numeric_cols) > 0:
                                col = numeric_cols[0]
                                axes[0, 0].hist(df[col].dropna(), bins=20, alpha=0.7, color='#667eea')
                                axes[0, 0].set_title(f'{col} åˆ†å¸ƒ')
                                axes[0, 0].set_xlabel(col)
                                axes[0, 0].set_ylabel('é¢‘æ•°')
                            
                            # ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ•°å€¼åˆ—ï¼‰
                            if len(numeric_cols) > 1:
                                corr_matrix = df[numeric_cols].corr()
                                im = axes[0, 1].imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)
                                axes[0, 1].set_xticks(range(len(corr_matrix.columns)))
                                axes[0, 1].set_yticks(range(len(corr_matrix.columns)))
                                axes[0, 1].set_xticklabels(corr_matrix.columns, rotation=45)
                                axes[0, 1].set_yticklabels(corr_matrix.columns)
                                axes[0, 1].set_title('ç›¸å…³æ€§çŸ©é˜µ')
                                plt.colorbar(im, ax=axes[0, 1])
                            
                            # åˆ†ç±»åˆ—çš„è®¡æ•°
                            categorical_cols = df.select_dtypes(include=['object']).columns
                            if len(categorical_cols) > 0:
                                col = categorical_cols[0]
                                value_counts = df[col].value_counts().head(10)
                                axes[1, 0].bar(range(len(value_counts)), value_counts.values, color='#764ba2')
                                axes[1, 0].set_xticks(range(len(value_counts)))
                                axes[1, 0].set_xticklabels(value_counts.index, rotation=45, ha='right')
                                axes[1, 0].set_title(f'{col} è®¡æ•°')
                                axes[1, 0].set_ylabel('æ•°é‡')
                            
                            # æ•£ç‚¹å›¾ï¼ˆå¦‚æœæœ‰ä¸¤ä¸ªæ•°å€¼åˆ—ï¼‰
                            if len(numeric_cols) >= 2:
                                axes[1, 1].scatter(df[numeric_cols[0]], df[numeric_cols[1]], alpha=0.6, color='#667eea')
                                axes[1, 1].set_xlabel(numeric_cols[0])
                                axes[1, 1].set_ylabel(numeric_cols[1])
                                axes[1, 1].set_title(f'{numeric_cols[0]} vs {numeric_cols[1]}')
                            
                            plt.tight_layout()
                            st.pyplot(fig)
                        except Exception as e:
                            st.warning(f"Error generating visualization: {str(e)}")

                return {
                    "success": True,
                    "response": ai_response,
                    "generation_time": generation_time
                }
            else:
                return {"success": False, "error": "DeepSeek API call failed, please check API key and network connection"}
                
        except Exception as e:
            return {"success": False, "error": str(e)}



    def analyze_document_structure(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£ç»“æ„ï¼Œè¯†åˆ«å†œä¸šé¢†åŸŸæ–‡æ¡£ç±»å‹ä¸è¦ç´ """
        analysis = {
            "document_type": "æœªçŸ¥",
            "data_types": [],
            "key_metrics": [],
            "time_periods": [],
            "categories": [],
            "confidence": 0.0
        }

        # è¯†åˆ«å†œä¸šæ–‡æ¡£ç±»å‹
        if any(k in text for k in ["å•äº§", "äº©äº§", "t/ha", "kg/ha", "æ’­ç§é¢ç§¯", "æ”¶è·é¢ç§¯", "äº§é‡"]):
            analysis["document_type"] = "ç§æ¤ä¸šç”Ÿäº§æŠ¥å‘Š"
            analysis["data_types"].extend(["é¢ç§¯", "äº§é‡", "å•äº§", "è¶‹åŠ¿"])
        elif any(k in text for k in ["å‡ºæ ", "å­˜æ ", "å¢é‡", "æ—¥å¢é‡", "æ–™è‚‰æ¯”", "å…ç–«"]):
            analysis["document_type"] = "ç•œç‰§ä¸šç”Ÿäº§æŠ¥å‘Š"
            analysis["data_types"].extend(["å¤´æ•°", "é‡é‡", "è½¬æ¢ç‡", "å…ç–«"])
        elif any(k in text for k in ["é™é›¨", "é™æ°´", "mm", "ç§¯æ¸©", "å¹²æ—±", "NDVI", "é¥æ„Ÿ"]):
            analysis["document_type"] = "æ°”å€™ä¸é¥æ„Ÿç›‘æµ‹"
            analysis["data_types"].extend(["é™é›¨", "æ¸©åº¦", "æŒ‡æ•°", "æ—¶é—´åºåˆ—"])
        elif any(k in text for k in ["æˆæœ¬", "é‡‡è´­", "ä»·æ ¼", "ä¿é™©", "èµ”ä»˜", "åˆ©æ¶¦", "æ¯›åˆ©ç‡"]):
            analysis["document_type"] = "å†œä¸šè´¢åŠ¡/ä¾›åº”é“¾æŠ¥å‘Š"
            analysis["data_types"].extend(["é‡‘é¢", "æ¯”ç‡", "å¯¹æ¯”", "ä»·æ ¼è¶‹åŠ¿"])

        # æå–å…³é”®æŒ‡æ ‡
        import re
        # æŸ¥æ‰¾æ•°å­—æ¨¡å¼ï¼ˆæ”¯æŒå¸¦å•ä½ï¼‰
        numbers = re.findall(r'[\d,]+\.?\d*\s*(?:t/ha|kg/ha|kg|t|å¨|å…¬æ–¤|å…ƒ/æ–¤|å…ƒ/å¨|mm)?', text)
        analysis["key_metrics"] = numbers[:10]  # å–å‰10ä¸ªæ•°å­—

        # æŸ¥æ‰¾æ—¶é—´æ¨¡å¼
        time_patterns = re.findall(r'\d{4}å¹´|\d{1,2}æœˆ|\d{1,2}æ—¥|Q[1-4]', text)
        analysis["time_periods"] = list(set(time_patterns))

        # æŸ¥æ‰¾åˆ†ç±»ä¿¡æ¯
        category_patterns = re.findall(r'[A-Za-z\u4e00-\u9fff]+[ï¼š:]\s*[\d,]+', text)
        analysis["categories"] = category_patterns[:5]

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆå†œä¸šåœºæ™¯ç¨å¾®æé«˜å…³é”®æŒ‡æ ‡æƒé‡ï¼‰
        confidence = min(len(analysis["key_metrics"]) * 0.12 +
                         len(analysis["time_periods"]) * 0.18 +
                         len(analysis["categories"]) * 0.1, 1.0)
        analysis["confidence"] = confidence

        return analysis

    def extract_data_points(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ•°æ®ç‚¹ç”¨äºç”Ÿæˆå›¾è¡¨ï¼ˆå¢å¼ºå†œä¸šå•ä½è¯†åˆ«ï¼‰"""
        data_points = []

        import re

        # æå–æ•°å€¼å’Œæ ‡ç­¾
        patterns = [
            r'([A-Za-z\u4e00-\u9fff]+)[ï¼š:]\s*([\d,]+\.?\d*)\s*(t/ha|kg/ha|kg|t|å¨|å…¬æ–¤|mm|%)?',
            r'([A-Za-z\u4e00-\u9fff]+)\s*([\d,]+\.?\d*)\s*(%)',
            r'([A-Za-z\u4e00-\u9fff]+)\s*ä¸º\s*([\d,]+\.?\d*)\s*(t/ha|kg/ha|kg|t|å¨|å…¬æ–¤|mm|%)?'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if len(match) == 3:
                    label, value, unit = match
                else:
                    label, value = match
                    unit = None
                try:
                    # æ¸…ç†æ•°å€¼
                    clean_value = float(value.replace(',', ''))
                    if clean_value > 0:  # åªä¿ç•™æ­£æ•°
                        data_points.append({
                            "label": label.strip(),
                            "value": clean_value,
                            "type": unit or "æ•°å€¼"
                        })
                except ValueError:
                    continue

        # å»é‡å¹¶æ’åº
        seen = set()
        unique_points = []
        for point in data_points:
            key = point["label"]
            if key not in seen:
                seen.add(key)
                unique_points.append(point)

        # æŒ‰æ•°å€¼æ’åº
        unique_points.sort(key=lambda x: x["value"], reverse=True)

        return unique_points[:10]  # è¿”å›å‰10ä¸ªæ•°æ®ç‚¹

    def generate_charts(self, data_points: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå›¾è¡¨æ•°æ®"""
        charts = []

        if not data_points:
            return charts

        # ç”ŸæˆæŸ±çŠ¶å›¾æ•°æ®
        if len(data_points) >= 2:
            bar_chart = {
                "type": "bar",
                "title": "æ•°æ®å¯¹æ¯”æŸ±çŠ¶å›¾",
                "data": {
                    "labels": [point["label"] for point in data_points[:8]],
                    "values": [point["value"] for point in data_points[:8]]
                }
            }
            charts.append(bar_chart)

        # ç”Ÿæˆé¥¼å›¾æ•°æ®ï¼ˆå‰5ä¸ªï¼‰
        if len(data_points) >= 3:
            pie_data = data_points[:5]
            total = sum(point["value"] for point in pie_data)
            pie_chart = {
                "type": "pie",
                "title": "æ•°æ®åˆ†å¸ƒé¥¼å›¾",
                "data": {
                    "labels": [point["label"] for point in pie_data],
                    "values": [point["value"] for point in pie_data],
                    "percentages": [round(point["value"] / total * 100, 1) for point in pie_data]
                }
            }
            charts.append(pie_chart)

        # ç”Ÿæˆè¶‹åŠ¿å›¾ï¼ˆå¦‚æœæœ‰æ—¶é—´æ•°æ®ï¼‰
        if len(data_points) >= 4:
            line_chart = {
                "type": "line",
                "title": "æ•°æ®è¶‹åŠ¿å›¾",
                "data": {
                    "labels": [point["label"] for point in data_points[:6]],
                    "values": [point["value"] for point in data_points[:6]]
                }
            }
            charts.append(line_chart)

        return charts

    def create_smart_report(self, analysis: Dict, charts: List[Dict], filename: str) -> str:
        """ç”Ÿæˆæ™ºèƒ½æŠ¥å‘Šï¼ˆåŠ å…¥å†œä¸šæ´å¯Ÿä¸KPIï¼‰"""
        report = f"# ğŸ“Š Agribusiness Smart Analysis Report\n\n"
        report += f"**File name**: {filename}\n\n"
        report += f"**Document type**: {analysis['document_type']}\n\n"
        report += f"**Confidence**: {analysis['confidence']:.1%}\n\n"

        # å†œä¸šKPIï¼ˆä»å…¨æ–‡æ™ºèƒ½æå–ï¼‰
        agrikpis = self.compute_agribusiness_kpis(analysis.get('full_text', '')) if isinstance(analysis, dict) else {}
        if agrikpis:
            report += "## ğŸŒ¾ Agribusiness KPIs\n\n"
            for k, v in agrikpis.items():
                report += f"- {k}: {v}\n"
            report += "\n"

        # å¤©æ°”æ‘˜è¦ï¼ˆå¦‚æœå·²è·å–ï¼‰
        if getattr(self, 'latest_weather', None):
            ws = self.latest_weather.get('summary', {})
            report += "## â˜ï¸ Climate summary (next 7 days)\n\n"
            if ws:
                if ws.get('7d_total_rain_mm') is not None:
                    report += f"- Total rainfall: {ws['7d_total_rain_mm']} mm\n"
                if ws.get('avg_tmax') is not None:
                    report += f"- Avg Tmax: {ws['avg_tmax']} Â°C\n"
                if ws.get('avg_tmin') is not None:
                    report += f"- Avg Tmin: {ws['avg_tmin']} Â°C\n"
            report += "\n"

        # é¥æ„Ÿæ‘˜è¦ï¼ˆå¦‚æœå·²è·å–ï¼‰
        if getattr(self, 'latest_remote_sensing', None):
            rs = self.latest_remote_sensing.get('summary', {})
            report += "## ğŸ›°ï¸ Remote sensing summary\n\n"
            if rs:
                if rs.get('ndvi_avg') is not None:
                    report += f"- NDVI average: {rs['ndvi_avg']}\n"
                if rs.get('evi_avg') is not None:
                    report += f"- EVI average: {rs['evi_avg']}\n"
                if rs.get('ndvi_last') is not None:
                    report += f"- Latest NDVI: {rs['ndvi_last']}\n"
                if rs.get('evi_last') is not None:
                    report += f"- Latest EVI: {rs['evi_last']}\n"
            report += "\n"

        # æ¨¡æ¿æŠ½å–ç»“æœ
        structured = self.extract_agri_structured_fields(analysis.get('full_text', '')) if isinstance(analysis,
                                                                                                      dict) else {}
        if structured:
            report += "## ğŸ—‚ï¸ Structured fields (template extraction)\n\n"
            for k, v in structured.items():
                report += f"- {k}: {v}\n"
            report += "\n"

        # Key metrics
        if analysis['key_metrics']:
            report += "## ğŸ”¢ Key metrics\n\n"
            for i, metric in enumerate(analysis['key_metrics'][:5], 1):
                report += f"{i}. {metric}\n"
            report += "\n"

        # Time periods
        if analysis['time_periods']:
            report += "## ğŸ“… Time periods\n\n"
            report += f"Detected time info: {', '.join(analysis['time_periods'])}\n\n"

        # Categories
        if analysis['categories']:
            report += "## ğŸ“‹ Categories\n\n"
            for category in analysis['categories']:
                report += f"- {category}\n"
            report += "\n"

        # Visualization notes
        if charts:
            report += "## ğŸ“ˆ Data visualization\n\n"
            for chart in charts:
                report += f"### {chart['title']}\n\n"
                if chart['type'] == 'bar':
                    report += "Bar chart shows value comparison across categories to spot highs and lows.\n\n"
                elif chart['type'] == 'pie':
                    report += "Pie chart shows proportion distribution for intuitive share comparison.\n\n"
                elif chart['type'] == 'line':
                    report += "Line chart shows temporal trends to identify growth or decline patterns.\n\n"

        # Suggestions
        report += "## ğŸ’¡ Suggestions\n\n"
        if analysis['document_type'] in ["ç§æ¤ä¸šç”Ÿäº§æŠ¥å‘Š", "ç•œç‰§ä¸šç”Ÿäº§æŠ¥å‘Š"]:
            report += "- Track trends of key KPIs (yield, rainfall, FCR).\n"
            report += "- Compare fields/lots or herds to find outliers.\n"
            report += "- Plan interventions (fertigation, pest control) based on thresholds.\n"
        elif analysis['document_type'] in ["å†œä¸šè´¢åŠ¡/ä¾›åº”é“¾æŠ¥å‘Š"]:
            report += "- Monitor margins and price trends.\n"
            report += "- Optimize cost structure and inventory turnover.\n"
            report += "- Manage risk with insurance/hedging where applicable.\n"
        else:
            report += "- Keep data updated regularly.\n"
            report += "- Focus on KPI trends and anomalies.\n"
            report += "- Apply data-driven decisions.\n"

        return report

    def compute_agribusiness_kpis(self, text: str) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™å¿«é€Ÿæå–å†œä¸šå¸¸è§KPIï¼ˆè½»é‡å ä½ï¼Œå¯åç»­æ¢æ¨¡å‹ï¼‰"""
        if not text:
            return {}
        import re
        kpis: Dict[str, Any] = {}
        try:
            # å•äº§ï¼ˆæ”¯æŒ kg/ha, t/ha, äº©äº§ï¼‰
            m = re.search(r'(å•äº§|äº©äº§)[:ï¼š]?\s*([\d,.]+)\s*(kg/ha|t/ha|å…¬æ–¤/äº©|æ–¤/äº©|å¨/å…¬é¡·)?', text)
            if m:
                kpis['å•äº§'] = f"{m.group(2)} {m.group(3) or ''}".strip()

            # é¢ç§¯ï¼ˆäº©ã€å…¬é¡·ï¼‰
            m = re.search(r'(æ’­ç§é¢ç§¯|æ”¶è·é¢ç§¯|é¢ç§¯)[:ï¼š]?\s*([\d,.]+)\s*(äº©|å…¬é¡·|ha)', text)
            if m:
                kpis['é¢ç§¯'] = f"{m.group(2)} {m.group(3)}"

            # é™é›¨é‡ï¼ˆmmï¼‰
            m = re.search(r'(é™é›¨|é™æ°´|ç´¯è®¡é™é›¨|ç´¯è®¡é™æ°´)[:ï¼š]?\s*([\d,.]+)\s*mm', text)
            if m:
                kpis['ç´¯è®¡é™é›¨'] = f"{m.group(2)} mm"

            # æˆæœ¬ä¸åˆ©æ¶¦
            m = re.search(r'(æ€»æˆæœ¬|æˆæœ¬)[:ï¼š]?\s*([\d,.]+)', text)
            if m:
                kpis['æˆæœ¬'] = m.group(2)
            m = re.search(r'(åˆ©æ¶¦|æ¯›åˆ©|æ¯›åˆ©ç‡)[:ï¼š]?\s*([\d,.]+)\s*(%)?', text)
            if m:
                kpis['åˆ©æ¶¦/æ¯›åˆ©'] = f"{m.group(2)}{m.group(3) or ''}"

            # ç•œç‰§å…³é”®æŒ‡æ ‡
            m = re.search(r'(å‡ºæ |å­˜æ )[:ï¼š]?\s*([\d,.]+)\s*(å¤´|åª)?', text)
            if m:
                kpis[m.group(1)] = f"{m.group(2)} {m.group(3) or ''}".strip()
            m = re.search(r'(æ–™è‚‰æ¯”|FCR)[:ï¼š]?\s*([\d,.]+)', text)
            if m:
                kpis['æ–™è‚‰æ¯”'] = m.group(2)

            # é¥æ„ŸæŒ‡æ•°
            m = re.search(r'(NDVI|EVI)[:ï¼š]?\s*([\d,.]+)', text)
            if m:
                kpis[m.group(1)] = m.group(2)
        except Exception:
            pass
        return kpis

    def calculate_checksum(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def get_file_type(self, filename: str) -> str:
        """è·å–æ–‡ä»¶ç±»å‹"""
        mime_type, _ = mimetypes.guess_type(filename)
        if mime_type:
            return mime_type.split('/')[0]
        return 'unknown'

    def upload_file(self, uploaded_file, folder_id: Optional[int] = None) -> Dict[str, Any]:
        """ä¸Šä¼ æ–‡ä»¶"""
        try:
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            timestamp = int(time.time())
            filename = f"{timestamp}_{uploaded_file.name}"
            file_path = self.storage_dir / filename

            # ä¿å­˜æ–‡ä»¶
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            # è®¡ç®—æ–‡ä»¶ä¿¡æ¯
            file_size = file_path.stat().st_size
            checksum = self.calculate_checksum(str(file_path))
            file_type = self.get_file_type(uploaded_file.name)

            # ä¿å­˜åˆ°æ•°æ®åº“
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO files (filename, file_path, file_size, file_type, folder_id, checksum)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (uploaded_file.name, str(file_path), file_size, file_type, folder_id, checksum))
            conn.commit()
            conn.close()

            return {
                "success": True,
                "filename": uploaded_file.name,
                "file_size": file_size,
                "file_type": file_type
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    def get_files(self, folder_id: Optional[int] = None) -> List[Dict[str, Any]]:
        """è·å–æ–‡ä»¶åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        if folder_id is None:
            # Query files with folder_id IS NULL
            cursor.execute('''
                SELECT id, filename, file_size, file_type, upload_time, is_cached
                FROM files WHERE folder_id IS NULL
                ORDER BY upload_time DESC
            ''')
            print(f"[DEBUG] get_files: Querying files with folder_id IS NULL")
        else:
            cursor.execute('''
                SELECT id, filename, file_size, file_type, upload_time, is_cached
                FROM files WHERE folder_id = ?
                ORDER BY upload_time DESC
            ''', (folder_id,))

        files = []
        for row in cursor.fetchall():
            files.append({
                "id": row[0],
                "filename": row[1],
                "file_size": row[2],
                "file_type": row[3],
                "upload_time": row[4],
                "is_cached": bool(row[5])
            })

        print(f"[DEBUG] get_files: folder_id={folder_id}, found {len(files)} files")
        if files:
            print(f"[DEBUG] get_files: First file - ID: {files[0]['id']}, Name: {files[0]['filename']}, Type: {files[0]['file_type']}")

        conn.close()
        return files
    
    def get_file_by_id(self, file_id: int) -> Optional[Dict[str, Any]]:
        """é€šè¿‡æ–‡ä»¶IDè·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆä¸ä¾èµ–æ–‡ä»¶å¤¹ï¼‰"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                SELECT id, filename, file_size, file_type, upload_time, is_cached, folder_id, file_path
                FROM files WHERE id = ?
            ''', (file_id,))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                print(f"[DEBUG] get_file_by_id: Found file - ID: {result[0]}, Name: {result[1]}, Path: {result[7]}")
                return {
                    "id": result[0],
                    "filename": result[1],
                    "file_size": result[2],
                    "file_type": result[3],
                    "upload_time": result[4],
                    "is_cached": bool(result[5]),
                    "folder_id": result[6],
                    "file_path": result[7]
                }
            else:
                print(f"[DEBUG] get_file_by_id: File not found - ID: {file_id}")
            return None
        except Exception as e:
            print(f"[DEBUG] get_file_by_id é”™è¯¯: {str(e)}")
            import traceback
            print(f"[DEBUG] get_file_by_id é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            return None

    def create_folder(self, folder_name: str, parent_folder_id: Optional[int] = None) -> Dict[str, Any]:
        """åˆ›å»ºæ–‡ä»¶å¤¹"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO folders (folder_name, parent_folder_id)
                VALUES (?, ?)
            ''', (folder_name, parent_folder_id))
            conn.commit()
            folder_id = cursor.lastrowid
            conn.close()

            return {"success": True, "folder_id": folder_id}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def search_files(self, query: str, file_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡ä»¶"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        if file_type:
            cursor.execute('''
                SELECT id, filename, file_size, file_type, upload_time, is_cached
                FROM files 
                WHERE filename LIKE ? AND file_type = ?
                ORDER BY upload_time DESC
            ''', (f"%{query}%", file_type))
        else:
            cursor.execute('''
                SELECT id, filename, file_size, file_type, upload_time, is_cached
                FROM files 
                WHERE filename LIKE ?
                ORDER BY upload_time DESC
            ''', (f"%{query}%",))

        files = []
        for row in cursor.fetchall():
            files.append({
                "id": row[0],
                "filename": row[1],
                "file_size": row[2],
                "file_type": row[3],
                "upload_time": row[4],
                "is_cached": bool(row[5])
            })

        conn.close()
        return files

    def preview_file(self, file_id: int) -> Optional[bytes]:
        """é¢„è§ˆæ–‡ä»¶"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT file_path, file_type FROM files WHERE id = ?', (file_id,))
            result = cursor.fetchone()
            conn.close()

            if not result:
                print(f"[DEBUG] preview_file: File not found in database - ID: {file_id}")
                return None

            file_path, file_type = result
            print(f"[DEBUG] preview_file: Attempting to read file - ID: {file_id}, Path: {file_path}")

            try:
                import os
                if not os.path.exists(file_path):
                    print(f"[DEBUG] preview_file: File path does not exist - {file_path}")
                    return None
                
                with open(file_path, 'rb') as f:
                    data = f.read()
                    print(f"[DEBUG] preview_file: Successfully read {len(data)} bytes from file")
                    return data
            except Exception as e:
                print(f"[DEBUG] preview_file: Error reading file - {str(e)}")
                import traceback
                print(f"[DEBUG] preview_file: Error stack:\n{traceback.format_exc()}")
                return None
        except Exception as e:
            print(f"[DEBUG] preview_file: Database error - {str(e)}")
            import traceback
            print(f"[DEBUG] preview_file: Error stack:\n{traceback.format_exc()}")
            return None

    def cache_file(self, file_id: int) -> bool:
        """ç¼“å­˜æ–‡ä»¶åˆ°æœ¬åœ°"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT file_path, filename FROM files WHERE id = ?', (file_id,))
            result = cursor.fetchone()

            if result:
                file_path, filename = result
                cache_path = self.cache_dir / filename
                shutil.copy2(file_path, cache_path)

                # æ›´æ–°æ•°æ®åº“
                cursor.execute('UPDATE files SET is_cached = TRUE WHERE id = ?', (file_id,))
                conn.commit()
                conn.close()
                return True
        except:
            pass
        return False

    def format_file_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes == 0:
            return "0B"
        size_names = ["B", "KB", "MB", "GB", "TB"]
        i = 0
        while size_bytes >= 1024 and i < len(size_names) - 1:
            size_bytes /= 1024.0
            i += 1
        return f"{size_bytes:.1f}{size_names[i]}"

    def get_file_icon(self, file_type: str) -> str:
        """è·å–æ–‡ä»¶ç±»å‹å›¾æ ‡"""
        icons = {
            'image': 'ğŸ–¼ï¸',
            'application': 'ğŸ“„',
            'text': 'ğŸ“',
            'video': 'ğŸ¥',
            'audio': 'ğŸµ',
            'unknown': 'ğŸ“'
        }
        return icons.get(file_type, 'ğŸ“')

    def upload_file_with_resume(self, uploaded_file, folder_id: Optional[int] = None, chunk_size: int = 1024 * 1024) -> \
    Dict[str, Any]:
        """å¸¦æ–­ç‚¹ç»­ä¼ çš„æ–‡ä»¶ä¸Šä¼ """
        try:
            filename = uploaded_file.name
            file_size = len(uploaded_file.getbuffer())

            # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä¸Šä¼ 
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                SELECT id, uploaded_size, checksum FROM upload_progress 
                WHERE filename = ? AND total_size = ?
                ORDER BY upload_time DESC LIMIT 1
            ''', (filename, file_size))

            progress_record = cursor.fetchone()

            if progress_record:
                # æ–­ç‚¹ç»­ä¼ 
                progress_id, uploaded_size, stored_checksum = progress_record
                st.info(f"ğŸ”„ Resumable upload found, continue from {uploaded_size} bytes...")
            else:
                # æ–°ä¸Šä¼ 
                uploaded_size = 0
                progress_id = None
                stored_checksum = None

            # åˆ†å—ä¸Šä¼ 
            uploaded_file.seek(uploaded_size)
            current_size = uploaded_size

            progress_bar = st.progress(uploaded_size / file_size)
            status_text = st.empty()

            while current_size < file_size:
                chunk = uploaded_file.read(min(chunk_size, file_size - current_size))
                if not chunk:
                    break

                # è¿™é‡Œåº”è¯¥å°†chunkå‘é€åˆ°æœåŠ¡å™¨
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç›´æ¥å†™å…¥æœ¬åœ°æ–‡ä»¶
                temp_file_path = self.storage_dir / f"temp_{filename}"
                with open(temp_file_path, "ab") as f:
                    f.write(chunk)

                current_size += len(chunk)
                progress = current_size / file_size
                progress_bar.progress(progress)
                status_text.text(f"Uploading: {current_size}/{file_size} bytes ({progress * 100:.1f}%)")

                # æ›´æ–°è¿›åº¦åˆ°æ•°æ®åº“
                if progress_id:
                    cursor.execute('''
                        UPDATE upload_progress 
                        SET uploaded_size = ? 
                        WHERE id = ?
                    ''', (current_size, progress_id))
                else:
                    cursor.execute('''
                        INSERT INTO upload_progress (filename, total_size, uploaded_size, chunk_size, checksum)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (filename, file_size, current_size, chunk_size, stored_checksum))
                    progress_id = cursor.lastrowid

                conn.commit()

                # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
                time.sleep(0.1)

            # ä¸Šä¼ å®Œæˆï¼Œç§»åŠ¨æ–‡ä»¶åˆ°æœ€ç»ˆä½ç½®
            final_file_path = self.storage_dir / f"{int(time.time())}_{filename}"
            shutil.move(str(temp_file_path), str(final_file_path))

            # è®¡ç®—æ ¡éªŒå’Œ
            checksum = self.calculate_checksum(str(final_file_path))
            file_type = self.get_file_type(filename)

            # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“
            file_path_str = str(final_file_path)
            print(f"[DEBUG] upload_file_with_resume: Saving to database - filename: {filename}, file_path: {file_path_str}, file_size: {file_size}, file_type: {file_type}, folder_id: {folder_id}")
            
            cursor.execute('''
                INSERT INTO files (filename, file_path, file_size, file_type, folder_id, checksum)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (filename, file_path_str, file_size, file_type, folder_id, checksum))
            
            file_id = cursor.lastrowid
            print(f"[DEBUG] upload_file_with_resume: File saved to database - file_id: {file_id}, filename: {filename}, folder_id: {folder_id}, file_path: {file_path_str}")
            
            # Verify the file was saved correctly
            cursor.execute('SELECT file_path FROM files WHERE id = ?', (file_id,))
            saved_path = cursor.fetchone()
            if saved_path:
                print(f"[DEBUG] upload_file_with_resume: Verified saved file_path: {saved_path[0]}")
            else:
                print(f"[DEBUG] upload_file_with_resume: WARNING - Could not verify saved file_path!")

            # åˆ é™¤è¿›åº¦è®°å½•
            if progress_id:
                cursor.execute('DELETE FROM upload_progress WHERE id = ?', (progress_id,))

            conn.commit()
            conn.close()

            progress_bar.empty()
            status_text.empty()

            return {
                "success": True,
                "filename": filename,
                "file_size": file_size,
                "file_type": file_type,
                "checksum": checksum,
                "file_id": file_id,
                "folder_id": folder_id
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def get_upload_progress(self) -> List[Dict[str, Any]]:
        """è·å–ä¸Šä¼ è¿›åº¦åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT filename, total_size, uploaded_size, upload_time
            FROM upload_progress
            ORDER BY upload_time DESC
        ''')

        progress_list = []
        for row in cursor.fetchall():
            filename, total_size, uploaded_size, upload_time = row
            progress_list.append({
                "filename": filename,
                "total_size": total_size,
                "uploaded_size": uploaded_size,
                "progress": uploaded_size / total_size if total_size > 0 else 0,
                "upload_time": upload_time
            })

        conn.close()
        return progress_list

    def resume_upload(self, filename: str) -> Dict[str, Any]:
        """æ¢å¤ä¸Šä¼ """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT id, total_size, uploaded_size, chunk_size, checksum
            FROM upload_progress 
            WHERE filename = ?
            ORDER BY upload_time DESC LIMIT 1
        ''', (filename,))

        result = cursor.fetchone()
        if result:
            progress_id, total_size, uploaded_size, chunk_size, checksum = result
            return {
                "success": True,
                "progress_id": progress_id,
                "total_size": total_size,
                "uploaded_size": uploaded_size,
                "chunk_size": chunk_size,
                "checksum": checksum
            }
        else:
            return {"success": False, "error": "æœªæ‰¾åˆ°ä¸Šä¼ è¿›åº¦è®°å½•"}

    def cancel_upload(self, filename: str) -> bool:
        """å–æ¶ˆä¸Šä¼ """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('DELETE FROM upload_progress WHERE filename = ?', (filename,))
            conn.commit()
            conn.close()
            return True
        except:
            return False

    # ==================== AIåŠŸèƒ½æ–¹æ³• ====================
    def extract_excel_csv(self, file_id: int):
        """
        é€šè¿‡file_idè¯»å–Excel(.xlsx, .xls)æˆ–CSVæ–‡ä»¶ï¼Œè¿”å›Pandas DataFrame
        éæ”¯æŒç±»å‹/è¯»å–å¤±è´¥æ—¶è¿”å›Noneï¼Œå¹¶æ˜¾ç¤ºStreamlitæç¤º
        """
        # 1. ä»æ•°æ®åº“æŸ¥è¯¢æ–‡ä»¶ä¿¡æ¯
        conn = None
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            # æŸ¥è¯¢æ–‡ä»¶è·¯å¾„ã€ç±»å‹ã€æ–‡ä»¶åï¼ˆä¸æ•°æ®åº“è¡¨ç»“æ„å¯¹åº”ï¼‰
            cursor.execute(
                'SELECT file_path, file_type, filename FROM files WHERE id = ?',
                (file_id,)
            )
            result = cursor.fetchone()
            if not result:
                st.error("File not found in database (invalid file ID).")
                return None  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›None

            file_path, file_type, filename = result
            filename = filename.lower()  # ç»Ÿä¸€è½¬ä¸ºå°å†™ï¼Œé¿å…å¤§å°å†™åˆ¤æ–­é—®é¢˜

            # 2. æ ¡éªŒæ–‡ä»¶ç±»å‹ï¼ˆä»…æ”¯æŒExcelå’ŒCSVï¼‰
            if filename.endswith(('.xlsx', '.xls')):
                # 3. è¯»å–Excelæ–‡ä»¶
                try:
                    df = pd.read_excel(file_path)
                    if df.empty:
                        st.warning("The Excel file is empty.")
                        return None
                    return df
                except FileNotFoundError:
                    st.error(f"Excel file not found at path: {file_path}")
                except pd.errors.EmptyDataError:
                    st.error("Excel file contains no valid data.")
                except pd.errors.ParserError:
                    st.error("Failed to parse Excel file (may be corrupted).")
                except Exception as e:
                    st.error(f"Error reading Excel file: {str(e)}")

            elif filename.endswith('.csv'):
                # 3. è¯»å–CSVæ–‡ä»¶
                try:
                    # å°è¯•å¸¸ç”¨ç¼–ç ï¼Œé¿å…ä¸­æ–‡ä¹±ç å¯¼è‡´è¯»å–å¤±è´¥
                    df = pd.read_csv(file_path, encoding='utf-8')
                except UnicodeDecodeError:
                    # ç¼–ç é”™è¯¯æ—¶å°è¯•gbkï¼ˆé€‚åˆä¸­æ–‡ç¯å¢ƒï¼‰
                    try:
                        df = pd.read_csv(file_path, encoding='gbk')
                    except Exception as e:
                        st.error(f"CSV file encoding error: {str(e)}")
                        return None
                except FileNotFoundError:
                    st.error(f"CSV file not found at path: {file_path}")
                    return None
                except pd.errors.EmptyDataError:
                    st.error("CSV file contains no valid data.")
                    return None
                except pd.errors.ParserError:
                    st.error("Failed to parse CSV file (may be corrupted).")
                    return None
                except Exception as e:
                    st.error(f"Error reading CSV file: {str(e)}")
                    return None

                if df.empty:
                    st.warning("The CSV file is empty.")
                    return None
                return df

            else:
                # éExcel/CSVç±»å‹ï¼Œè¿”å›Noneï¼ˆå°†ç”±extract_text_from_fileå¤„ç†ï¼‰
                return None

        except sqlite3.Error as db_err:
            st.error(f"Database error: {str(db_err)} (failed to get file info)")
            return None
        finally:
            # ç¡®ä¿æ•°æ®åº“è¿æ¥å…³é—­
            if conn:
                conn.close()
        return None

    def extract_text_from_file(self, file_id: int) -> str:
        """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT file_path, file_type, filename FROM files WHERE id = ?', (file_id,))
        result = cursor.fetchone()
        conn.close()

        if not result:
            return ""

        file_path, file_type, filename = result
        extracted_text = ""

        try:
            if file_type == 'text' or filename.endswith('.txt'):
                # æ–‡æœ¬æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    extracted_text = f.read()

            elif file_type == 'application' and filename.endswith('.pdf'):
                # PDFæ–‡ä»¶
                if PDF_AVAILABLE and fitz is not None:
                    try:
                        doc = fitz.open(file_path)
                        for page in doc:
                            extracted_text += page.get_text()
                        doc.close()
                    except Exception as e:
                        st.warning(f"PDF reading failed: {str(e)}")
                # è‹¥ä¸å¯ç”¨åˆ™ä¿æŒä¸ºç©ºï¼Œåç»­ç»™å‡ºå‹å¥½å ä½

            elif file_type == 'application' and filename.endswith(('.xlsx', '.xls')):
                # Excelæ–‡ä»¶
                try:
                    df = pd.read_excel(file_path)
                    # ç¡®ä¿DataFrameä¸ä¸ºç©º
                    if not df.empty:
                        # å®‰å…¨åœ°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…numpy.str_é”™è¯¯
                        try:
                            extracted_text = df.to_string()
                        except Exception as str_error:
                            # å¦‚æœto_stringå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                            extracted_text = str(df.values.tolist())
                    else:
                        extracted_text = "Excel file is empty"
                except Exception as e:
                    st.warning(f"Excel reading failed: {str(e)}")
                    extracted_text = ""

            elif filename.endswith('.csv'):
                # CSVæ–‡ä»¶
                try:
                    df = pd.read_csv(file_path)
                    if not df.empty:
                        try:
                            extracted_text = df.to_string()
                        except Exception:
                            extracted_text = str(df.values.tolist())
                    else:
                        extracted_text = "CSV file is empty"
                except Exception as e:
                    st.warning(f"CSV reading failed: {str(e)}")
                    extracted_text = ""

            elif filename.endswith('.docx'):
                # DOCXï¼ˆå¯é€‰å¤„ç†ï¼‰
                try:
                    import docx  # type: ignore
                    doc = docx.Document(file_path)
                    paras = [p.text for p in doc.paragraphs if p.text]
                    extracted_text = "\n".join(paras)
                except Exception:
                    # æœªå®‰è£…æˆ–è§£æå¤±è´¥åˆ™å¿½ç•¥
                    pass

            elif file_type == 'image':
                # å›¾ç‰‡æ–‡ä»¶ - OCRè¯†åˆ«
                print(f"[DEBUG] å¼€å§‹å¤„ç†å›¾ç‰‡æ–‡ä»¶: {filename}")
                print(f"[DEBUG] æ–‡ä»¶è·¯å¾„: {file_path}")
                print(f"[DEBUG] OCRçŠ¶æ€ - OCR_AVAILABLE: {OCR_AVAILABLE}, TESSERACT: {TESSERACT_AVAILABLE}")
                
                if OCR_AVAILABLE and TESSERACT_AVAILABLE:
                    # å»¶è¿ŸåŠ è½½OCRæ¨¡å‹
                    if not self._load_ocr_model():
                        print("[DEBUG] OCRæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡OCRæå–")
                        extracted_text = ""
                    else:
                        # OCRæ¨¡å‹å·²åŠ è½½ï¼Œè¿›è¡Œè¯†åˆ«
                        print(f"[DEBUG] OCRæ¨¡å‹å·²åŠ è½½ï¼Œå¼€å§‹è¯†åˆ«å›¾ç‰‡: {file_path}")
                        try:
                            print("[DEBUG] è°ƒç”¨ _ocr_readtext()...")
                            results = self._ocr_readtext(file_path)
                            print(f"[DEBUG] OCRè¯†åˆ«å®Œæˆï¼Œè¿”å›ç»“æœæ•°é‡: {len(results) if results else 0}")
                            
                            if results and len(results) > 0:
                                print(f"[DEBUG] OCRè¯†åˆ«ç»“æœè¯¦æƒ…:")
                                for i, result in enumerate(results):
                                    print(f"  [{i+1}] ä½ç½®: {result[0]}, æ–‡å­—: {result[1]}, ç½®ä¿¡åº¦: {result[2]:.2f}")
                                
                                extracted_text = ' '.join([result[1] for result in results])
                                if extracted_text.strip():
                                    print(f"[DEBUG] âœ… OCRè¯†åˆ«æˆåŠŸï¼Œæå–çš„æ–‡å­—: {extracted_text[:100]}...")
                                    st.success(f"âœ… OCR recognition successful, recognized {len(results)} text regions")
                                else:
                                    extracted_text = ""  # OCRæ²¡æœ‰è¯†åˆ«åˆ°æ–‡å­—
                                    print("[DEBUG] âš ï¸ OCRè¯†åˆ«ç»“æœä¸ºç©ºå­—ç¬¦ä¸²")
                                    st.warning("âš ï¸ OCR did not recognize any text content")
                            else:
                                extracted_text = ""  # OCRæ²¡æœ‰è¯†åˆ«åˆ°æ–‡å­—
                                print("[DEBUG] âš ï¸ OCRæœªè¯†åˆ«åˆ°ä»»ä½•æ–‡å­—ï¼ˆresultsä¸ºç©ºï¼‰")
                                st.warning("âš ï¸ OCRæœªè¯†åˆ«åˆ°æ–‡å­—å†…å®¹")
                        except Exception as e:
                            print(f"[DEBUG] âŒ OCRè¯†åˆ«è¿‡ç¨‹å‡ºé”™: {str(e)}")
                            print(f"[DEBUG] é”™è¯¯ç±»å‹: {type(e).__name__}")
                            import traceback
                            print(f"[DEBUG] é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                            st.warning(f"OCR recognition failed: {str(e)}")
                            extracted_text = ""
                else:
                    # OCRä¸å¯ç”¨ï¼Œæç¤ºç”¨æˆ·
                    print(f"[DEBUG] OCRä¸å¯ç”¨ - OCR_AVAILABLE: {OCR_AVAILABLE}, TESSERACT: {TESSERACT_AVAILABLE}")
                    if not OCR_AVAILABLE or not TESSERACT_AVAILABLE:
                        st.warning("âš ï¸ OCR feature unavailable. Please install Tesseract OCR. See INSTALL_TESSERACT.md for details.")
                    extracted_text = ""

        except Exception as e:
            st.error(f"Text extraction failed: {str(e)}")

        # å…œåº•ï¼šä»æ— æ³•æå–æ–‡æœ¬æ—¶ï¼Œè¿”å›å ä½æ–‡æœ¬ï¼Œé¿å…AIæµç¨‹ç›´æ¥å¤±è´¥
        if not extracted_text:
            extracted_text = f"(No extractable text from file: {filename}. Try preview/download.)"

        return extracted_text

    def classify_industry(self, text: str) -> Dict[str, Any]:
        """ä½¿ç”¨çœŸæ­£çš„AIå¯¹æ–‡æ¡£è¿›è¡Œè¡Œä¸šåˆ†ç±»ï¼Œè¿”å›ä¸å·¥ä¸šè§†å›¾åŒ¹é…çš„æ ‡ç­¾"""
        if not text:
            return {"category": "Unclassified", "confidence": 0.0, "keywords": []}

        # æ–¹æ³•1: ä½¿ç”¨BERTæ¨¡å‹åˆ†ç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.text_classifier and len(text) > 10:
            try:
                # æˆªå–æ–‡æœ¬å‰512ä¸ªå­—ç¬¦ï¼ˆBERTé™åˆ¶ï¼‰
                text_sample = text[:512]
                result = self.text_classifier(text_sample)

                # å°†BERTç»“æœæ˜ å°„åˆ°æˆ‘ä»¬çš„è¡Œä¸šåˆ†ç±»
                bert_label = result[0]['label']
                bert_confidence = result[0]['score']

                # ç®€å•çš„æ ‡ç­¾æ˜ å°„ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
                label_mapping = {
                    'LABEL_0': 'ç§æ¤ä¸š',
                    'LABEL_1': 'ç•œç‰§ä¸š',
                    'LABEL_2': 'å†œèµ„ä¸åœŸå£¤',
                    'LABEL_3': 'å†œä¸šé‡‘è',
                    'LABEL_4': 'ä¾›åº”é“¾ä¸ä»“å‚¨',
                    'LABEL_5': 'æ°”å€™ä¸é¥æ„Ÿ',
                    'LABEL_6': 'å†œä¸šç‰©è”ç½‘'
                }

                mapped_category = label_mapping.get(bert_label, 'Unclassified')
                # è½¬æ¢ä¸ºè‹±æ–‡åˆ†ç±»åç§°
                eng_category = self._to_english_category(mapped_category)

                if eng_category != 'Unclassified':
                    return {
                        "category": eng_category,
                        "confidence": bert_confidence,
                        "keywords": self._extract_keywords_from_text(text),
                        "method": "BERT"
                    }
            except Exception as e:
                # Suppress noisy toast; fallback methods will be tried below
                pass

        # æ–¹æ³•2: ä½¿ç”¨æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ï¼ˆå¦‚æœå¯ç”¨ä¸”å·²è®­ç»ƒï¼‰
        if self.ml_classifier and self.ml_trained and len(text) > 20:
            try:
                X = [text]
                y_pred = self.ml_classifier.predict(X)
                y_proba = self.ml_classifier.predict_proba(X)

                categories = list(self.industry_keywords.keys())
                predicted_category = categories[y_pred[0]]
                confidence = y_proba[0].max()
                
                # å¦‚æœç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼Œç›´æ¥è¿”å›Unclassified
                if confidence < 0.1:
                    print(f"[DEBUG] classify_industry (ML): ç½®ä¿¡åº¦å¤ªä½ ({confidence:.2f})ï¼Œè¿”å›Unclassified")
                    return {"category": "Unclassified", "confidence": 0.0, "keywords": [], "method": "ML"}

                # è½¬æ¢ä¸ºè‹±æ–‡åˆ†ç±»åç§°
                eng_category = self._to_english_category(predicted_category)
                return {
                    "category": eng_category,
                    "confidence": confidence,
                    "keywords": self._extract_keywords_from_text(text),
                    "method": "ML"
                }
            except Exception as e:
                # Suppress noisy toast; fallback to rules
                pass

        # æ–¹æ³•3: æ™ºèƒ½å…³é”®è¯åŒ¹é…ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        words = jieba.lcut(text)
        category_scores = {}
        matched_keywords = {}

        for category, keywords in self.industry_keywords.items():
            score = 0
            matched = []

            # åŸºç¡€å…³é”®è¯åŒ¹é…
            for keyword in keywords:
                if keyword in text:
                    score += 1
                    matched.append(keyword)

            # åŒä¹‰è¯å’Œç›¸ä¼¼è¯åŒ¹é…
            synonyms = self._get_synonyms(category)
            for synonym in synonyms:
                if synonym in text:
                    score += 0.5
                    matched.append(synonym)

            # è¯é¢‘æƒé‡
            for keyword in keywords:
                count = text.count(keyword)
                if count > 1:
                    score += count * 0.2

            category_scores[category] = score
            matched_keywords[category] = matched

        if category_scores and max(category_scores.values()) > 0:
            best_category = max(category_scores, key=category_scores.get)
            max_score = category_scores[best_category]

            # æ”¹è¿›çš„ç½®ä¿¡åº¦è®¡ç®—
            total_keywords = len(self.industry_keywords[best_category])
            confidence = min(max_score / (total_keywords * 1.5), 1.0)

            # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä»0.1é™åˆ°0.05ï¼‰ï¼Œå…è®¸æ›´å¤šæ–‡ä»¶è¢«åˆ†ç±»
            if confidence < 0.1:
                print(f"[DEBUG] classify_industry: ç½®ä¿¡åº¦å¤ªä½ ({confidence:.2f})ï¼Œè¿”å›Unclassified")
                return {"category": "Unclassified", "confidence": 0.0, "keywords": [], "method": "å…³é”®è¯åŒ¹é…"}

            # è½¬æ¢ä¸ºè‹±æ–‡åˆ†ç±»åç§°
            eng_category = self._to_english_category(best_category)
            return {
                "category": eng_category,
                "confidence": confidence,
                "keywords": matched_keywords[best_category],
                "method": "æ™ºèƒ½å…³é”®è¯åŒ¹é…"
            }

        return {"category": "Unclassified", "confidence": 0.0, "keywords": [], "method": "æ— åŒ¹é…"}

    def _get_synonyms(self, category: str) -> List[str]:
        """è·å–è¡Œä¸šåˆ†ç±»çš„åŒä¹‰è¯"""
        synonyms_map = {
            "ç§æ¤ä¸š": ["ç§æ¤", "è€•ä½œ", "è‚²ç§§", "ç§»æ ½", "å¯†æ¤", "ç—…è™«å®³", "æ–½è‚¥", "çŒæº‰", "ç”°é—´ç®¡ç†", "ç‰ç±³", "é«˜ç²±",
                       "å°ç±³", "æœ¨è–¯", "èŠ±ç”Ÿ", "èŠéº»", "æ£‰èŠ±", "å¯å¯", "å’–å•¡"],
            "ç•œç‰§ä¸š": ["å…»æ®–", "é¥²å–‚", "å…ç–«", "é˜²ç–«", "ç¹è‚²", "æ–­å¥¶", "å‡ºæ ", "å­˜æ ", "å¢é‡"],
            "å†œèµ„ä¸åœŸå£¤": ["é…æ–¹æ–½è‚¥", "åœŸå£¤æ”¹è‰¯", "æ–½ç”¨é‡", "æœ‰æœºè‚¥", "å¾®é‡å…ƒç´ ", "åœŸå£¤å…»åˆ†"],
            "å†œä¸šé‡‘è": ["è´´ç°", "æˆä¿¡", "ä¿è´¹", "èµ”ä»˜", "æ‰¿ä¿", "é£æ§", "ä¿å•"],
            "ä¾›åº”é“¾ä¸ä»“å‚¨": ["å†·é“¾è¿è¾“", "æŸè€—ç‡", "æ‰¹æ¬¡è¿½æº¯", "åº“å®¹", "å‘¨è½¬ç‡", "åˆ†æ‹£"],
            "æ°”å€™ä¸é¥æ„Ÿ": ["é™é›¨", "æ°”æ¸©", "ç§¯æ¸©", "å¹²æ—±æŒ‡æ•°", "NDVI", "EVI", "é¥æ„Ÿ", "æ²™æ¼ è—è™«", "è‰åœ°è´ªå¤œè›¾"],
            "å†œä¸šç‰©è”ç½‘": ["å«æ°´ç‡", "EC", "æ»´çŒ", "å–·çŒ", "é˜€é—¨", "é˜ˆå€¼", "æŠ¥è­¦"]
        }
        return synonyms_map.get(category, [])

    def init_pretrained_classifier(self):
        """åˆå§‹åŒ–é¢„è®­ç»ƒçš„åˆ†ç±»å™¨"""
        if not self.ml_classifier:
            return False

        try:
            # ä½¿ç”¨é¢„å®šä¹‰çš„å…³é”®è¯ä½œä¸ºç‰¹å¾è¿›è¡Œè®­ç»ƒ
            X_train = []
            y_train = []

            # ä¸ºæ¯ä¸ªè¡Œä¸šç±»åˆ«åˆ›å»ºè®­ç»ƒæ ·æœ¬
            for category, keywords in self.industry_keywords.items():
                # ä¸ºæ¯ä¸ªå…³é”®è¯åˆ›å»ºè®­ç»ƒæ ·æœ¬
                for keyword in keywords:
                    # åˆ›å»ºåŒ…å«å…³é”®è¯çš„æ ·æœ¬æ–‡æœ¬
                    sample_text = f"è¿™æ˜¯ä¸€ä¸ªå…³äº{keyword}çš„æ–‡æ¡£ï¼Œæ¶‰åŠ{category}é¢†åŸŸçš„å†…å®¹ã€‚"
                    X_train.append(sample_text)
                    y_train.append(category)

                # æ·»åŠ åŒä¹‰è¯æ ·æœ¬
                synonyms = self._get_synonyms(category)
                for synonym in synonyms:
                    sample_text = f"è¿™æ˜¯ä¸€ä¸ªå…³äº{synonym}çš„æ–‡æ¡£ï¼Œæ¶‰åŠ{category}é¢†åŸŸçš„å†…å®¹ã€‚"
                    X_train.append(sample_text)
                    y_train.append(category)

            # è®­ç»ƒåˆ†ç±»å™¨
            if len(X_train) > 0:
                self.ml_classifier.fit(X_train, y_train)
                self.ml_trained = True
                return True
            else:
                return False

        except Exception as e:
            st.error(f"Failed to initialize pre-trained classifier: {str(e)}")
            return False

    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        try:
            # ä½¿ç”¨jiebaçš„TF-IDFæå–å…³é”®è¯
            keywords = jieba.analyse.extract_tags(text, topK=10, withWeight=False)
            return keywords
        except:
            # ç®€å•çš„å…³é”®è¯æå–
            words = jieba.lcut(text)
            word_count = Counter(words)
            stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ',
                          'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
            filtered_words = {word: count for word, count in word_count.items()
                              if len(word) > 1 and word not in stop_words and count > 1}
            return list(dict(sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:10]).keys())

    def extract_key_phrases(self, text: str, top_k: int = 10) -> List[str]:
        """æå–å…³é”®çŸ­è¯­"""
        if not text:
            return []

        try:
            # ä½¿ç”¨jiebaçš„TF-IDFæå–å…³é”®è¯
            keywords = jieba.analyse.extract_tags(text, topK=top_k, withWeight=False)
            return keywords
        except:
            # ç®€å•çš„å…³é”®è¯æå–
            words = jieba.lcut(text)
            word_count = Counter(words)
            # è¿‡æ»¤æ‰å•å­—ç¬¦å’Œå¸¸è§åœç”¨è¯
            stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ',
                          'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
            filtered_words = {word: count for word, count in word_count.items()
                              if len(word) > 1 and word not in stop_words and count > 1}
            return list(dict(sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:top_k]).keys())

    def generate_summary(self, text: str, max_length: int = 200) -> str:
        """Generate document summary (model first, fallback to rules)."""
        if not text:
            return "Unable to generate summary"

        # æ–¹æ³•1: ä½¿ç”¨T5æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.summarizer and len(text) > 50:
            try:
                # æˆªå–æ–‡æœ¬å‰1024ä¸ªå­—ç¬¦ï¼ˆT5é™åˆ¶ï¼‰
                text_sample = text[:1024]
                summary_result = self.summarizer(
                    text_sample,
                    max_length=min(max_length, 150),
                    min_length=30,
                    do_sample=False
                )

                if summary_result and len(summary_result) > 0:
                    ai_summary = summary_result[0]['summary_text']
                    return f"ğŸ¤– AI Summary: {ai_summary}"
            except Exception as e:
                st.warning(f"T5 summarization failed: {str(e)}")

        # æ–¹æ³•2: ä½¿ç”¨OpenAI GPTï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if OPENAI_AVAILABLE and len(text) > 100:
            try:
                # è¿™é‡Œéœ€è¦OpenAI APIå¯†é’¥
                # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºéœ€è¦APIå¯†é’¥
                pass
            except Exception as e:
                st.warning(f"OpenAI summarization failed: {str(e)}")

        # æ–¹æ³•3: æ™ºèƒ½å¥å­é€‰æ‹©ï¼ˆæ”¹è¿›çš„è§„åˆ™æ–¹æ³•ï¼‰
        try:
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„å¥å­é€‰æ‹©
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
            sentences = [s.strip() for s in sentences if s.strip() and len(s) > 10]

            if len(sentences) <= 2:
                return text[:max_length] + "..." if len(text) > max_length else text

            # é€‰æ‹©æœ€é‡è¦çš„å¥å­ï¼ˆåŸºäºé•¿åº¦å’Œå…³é”®è¯ï¼‰
            scored_sentences = []
            for i, sentence in enumerate(sentences):
                score = len(sentence)  # åŸºç¡€åˆ†æ•°ï¼šå¥å­é•¿åº¦

                # å…³é”®è¯åŠ åˆ†
                important_words = ['é‡è¦', 'å…³é”®', 'ä¸»è¦', 'æ ¸å¿ƒ', 'æ€»ç»“', 'ç»“è®º', 'ç»“æœ', 'å‘ç°']
                for word in important_words:
                    if word in sentence:
                        score += 20

                # ä½ç½®åŠ åˆ†ï¼ˆå¼€å¤´å’Œç»“å°¾çš„å¥å­æ›´é‡è¦ï¼‰
                if i < 2 or i >= len(sentences) - 2:
                    score += 10

                scored_sentences.append((score, sentence))

            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„2-3ä¸ªå¥å­
            scored_sentences.sort(reverse=True)
            selected_sentences = [s[1] for s in scored_sentences[:3]]

            summary = 'ã€‚'.join(selected_sentences)
            if len(summary) > max_length:
                summary = summary[:max_length] + "..."

            return f"ğŸ“ Smart summary: {summary}"
        except:
            # æ–¹æ³•4: ç®€å•æˆªå–ï¼ˆæœ€åå¤‡ç”¨ï¼‰
            return text[:max_length] + "..." if len(text) > max_length else text

    def _load_ocr_model(self):
        """æ£€æŸ¥OCRæ˜¯å¦å¯ç”¨ï¼ˆTesseractæ— éœ€åŠ è½½æ¨¡å‹ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦ç¦ç”¨OCR
        import os
        if os.getenv('DISABLE_OCR', '').lower() in ('1', 'true', 'yes'):
            print("[DEBUG] OCRå·²é€šè¿‡ç¯å¢ƒå˜é‡ç¦ç”¨")
            self.ocr_load_failed = True
            return False
        
        if self.ocr_load_failed:
            print("[DEBUG] OCRä¹‹å‰æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡é‡è¯•")
            return False
        
        if not OCR_AVAILABLE or not TESSERACT_AVAILABLE:
            print("[DEBUG] Tesseract OCRä¸å¯ç”¨")
            return False
        
        # Tesseractä¸éœ€è¦åŠ è½½æ¨¡å‹ï¼Œç›´æ¥å¯ç”¨
        print("[DEBUG] âœ… Tesseract OCRå¯ç”¨ï¼ˆæ— éœ€åŠ è½½æ¨¡å‹ï¼Œè½»é‡çº§ï¼‰")
        return True
    
    def _ocr_readtext(self, image_path: str):
        """OCRè¯†åˆ«æ¥å£ - ä½¿ç”¨Tesseract OCR"""
        if not self._load_ocr_model():
            return []
        
        try:
            # ä½¿ç”¨Tesseract OCR
            import pytesseract
            from PIL import Image
            
            # è¯»å–å›¾ç‰‡
            img = Image.open(image_path)
            
            # æ£€æµ‹è¯­è¨€
            import os
            lang = 'chi_sim+eng' if os.getenv('ENABLE_CHINESE_OCR', '').lower() in ('1', 'true', 'yes') else 'eng'
            
            # è¯†åˆ«æ–‡å­—
            text = pytesseract.image_to_string(img, lang=lang)
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼: [(bbox, text, confidence)]
            # Tesseractåªè¿”å›æ–‡å­—ï¼Œæ²¡æœ‰åæ ‡ä¿¡æ¯ï¼Œbboxè®¾ä¸ºNoneï¼Œconfidenceè®¾ä¸º1.0
            if text.strip():
                return [(None, text.strip(), 1.0)]
            return []
        except Exception as e:
            print(f"[DEBUG] Tesseract OCRè¯†åˆ«å¤±è´¥: {str(e)}")
            import traceback
            print(f"[DEBUG] é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            return []
    
    def extract_ocr_content(self, file_id: int) -> Optional[str]:
        """æå–å›¾ç‰‡æˆ–PDFçš„OCRå†…å®¹ï¼ˆç”¨äºä¿å­˜åˆ°æ•°æ®åº“ï¼‰"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT file_path, file_type, filename FROM files WHERE id = ?', (file_id,))
            result = cursor.fetchone()
            conn.close()

            if not result:
                return None

            file_path, file_type, filename = result
            
            # åªå¤„ç†å›¾ç‰‡å’ŒPDFæ–‡ä»¶
            if file_type != 'image' and not (file_type == 'application' and filename.endswith('.pdf')):
                return None

            ocr_content = None

            # å»¶è¿ŸåŠ è½½OCRæ¨¡å‹
            if not self._load_ocr_model():
                print("[DEBUG] extract_ocr_content: OCRæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡OCRæå–")
                return None

            # å¯¹äºPDFæ–‡ä»¶ï¼Œéœ€è¦è½¬æ¢ä¸ºå›¾ç‰‡åOCR
            if filename.endswith('.pdf') and PDF_AVAILABLE and fitz is not None:
                try:
                    doc = fitz.open(file_path)
                    all_ocr_text = []
                    
                    # é™åˆ¶PDFé¡µæ•°ï¼Œé¿å…å†…å­˜æº¢å‡º
                    max_pages = min(len(doc), 10)  # æœ€å¤šå¤„ç†10é¡µ
                    if len(doc) > max_pages:
                        print(f"[DEBUG] PDFæœ‰{len(doc)}é¡µï¼Œåªå¤„ç†å‰{max_pages}é¡µä»¥èŠ‚çœå†…å­˜")
                    
                    for page_num in range(max_pages):
                        page = doc[page_num]
                        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                        img_data = pix.tobytes("png")
                        
                        import tempfile
                        temp_img = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
                        temp_img.write(img_data)
                        temp_img.close()
                        
                        try:
                            page_results = self._ocr_readtext(temp_img.name)
                            if page_results and len(page_results) > 0:
                                page_text = ' '.join([result[1] for result in page_results])
                                all_ocr_text.append(f"Page {page_num + 1}:\n{page_text}")
                        finally:
                            try:
                                os.unlink(temp_img.name)
                            except:
                                pass
                    
                    doc.close()
                    
                    if all_ocr_text:
                        ocr_content = '\n\n'.join(all_ocr_text)
                except Exception as e:
                    print(f"[DEBUG] extract_ocr_content: PDF OCRå¤±è´¥: {str(e)}")
            
            # å¯¹äºå›¾ç‰‡æ–‡ä»¶ï¼Œç›´æ¥OCR
            elif file_type == 'image':
                try:
                    # æ£€æŸ¥å›¾ç‰‡å¤§å°å’Œå°ºå¯¸ï¼Œå¦‚æœå¤ªå¤§åˆ™ç¼©æ”¾
                    from PIL import Image
                    img = Image.open(file_path)
                    img_width, img_height = img.size
                    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
                    
                    print(f"[DEBUG] extract_ocr_content: å›¾ç‰‡å°ºå¯¸: {img_width}x{img_height}, æ–‡ä»¶å¤§å°: {file_size_mb:.2f}MB")
                    
                    # å¦‚æœå›¾ç‰‡å¤ªå¤§ï¼Œè¿›è¡Œç¼©æ”¾
                    max_dimension = 2000  # æœ€å¤§å°ºå¯¸2000åƒç´ 
                    max_file_size_mb = 5  # æœ€å¤§æ–‡ä»¶å¤§å°5MB
                    
                    ocr_file_path = file_path
                    temp_img_path = None
                    
                    if img_width > max_dimension or img_height > max_dimension or file_size_mb > max_file_size_mb:
                        print(f"[DEBUG] extract_ocr_content: å›¾ç‰‡è¿‡å¤§ï¼Œè¿›è¡Œç¼©æ”¾...")
                        
                        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                        scale = min(max_dimension / img_width, max_dimension / img_height)
                        new_width = int(img_width * scale)
                        new_height = int(img_height * scale)
                        
                        # ç¼©æ”¾å›¾ç‰‡
                        img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                        
                        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                        import tempfile
                        temp_img_path = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
                        temp_img_path.close()
                        img_resized.save(temp_img_path.name, 'PNG')
                        
                        ocr_file_path = temp_img_path.name
                        print(f"[DEBUG] extract_ocr_content: å›¾ç‰‡å·²ç¼©æ”¾è‡³: {new_width}x{new_height}")
                    
                    try:
                        results = self._ocr_readtext(ocr_file_path)
                        if results and len(results) > 0:
                            ocr_content = ' '.join([result[1] for result in results])
                    except MemoryError as e:
                        print(f"[DEBUG] extract_ocr_content: å›¾ç‰‡OCRå†…å­˜ä¸è¶³: {str(e)}")
                        ocr_content = None
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        if temp_img_path and os.path.exists(temp_img_path.name):
                            try:
                                os.unlink(temp_img_path.name)
                            except:
                                pass
                except MemoryError as e:
                    print(f"[DEBUG] extract_ocr_content: å›¾ç‰‡å¤„ç†å†…å­˜ä¸è¶³: {str(e)}")
                    ocr_content = None
                except Exception as e:
                    print(f"[DEBUG] extract_ocr_content: å›¾ç‰‡OCRå¤±è´¥: {str(e)}")
                    ocr_content = None
            
            return ocr_content
        except Exception as e:
            print(f"[DEBUG] extract_ocr_content: é”™è¯¯: {str(e)}")
            return None

    def analyze_file_with_ai(self, file_id: int) -> Dict[str, Any]:
        """ä½¿ç”¨DeepSeek AIåˆ†ææ–‡ä»¶"""
        try:
            # æå–æ–‡æœ¬
            extracted_text = self.extract_text_from_file(file_id)

            if not extracted_text:
                return {"success": False, "error": "æ— æ³•æå–æ–‡ä»¶æ–‡æœ¬å†…å®¹"}
            
            # å¯¹äºå›¾ç‰‡å’ŒPDFæ–‡ä»¶ï¼Œæå–å¹¶ä¿å­˜OCRå†…å®¹
            ocr_content = None
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT file_type, filename FROM files WHERE id = ?', (file_id,))
            file_info = cursor.fetchone()
            conn.close()
            
            if file_info:
                file_type, filename = file_info
                if file_type == 'image' or (file_type == 'application' and filename.endswith('.pdf')):
                    ocr_content = self.extract_ocr_content(file_id)
                    print(f"[DEBUG] analyze_file_with_ai: OCRå†…å®¹æå–å®Œæˆï¼Œé•¿åº¦: {len(ocr_content) if ocr_content else 0}")

            # å¦‚æœé…ç½®äº†DeepSeek APIï¼Œä½¿ç”¨AIåˆ†æ
            if self.deepseek_api_key:
                # æ„å»ºåˆ†ææç¤ºè¯ - æ˜ç¡®è¦æ±‚è¿”å›è¡Œä¸šåˆ†ç±»
                system_prompt = """You are a professional document analysis assistant. Please analyze the user's uploaded file and provide the following information in a structured format:
1. File type and main content overview
2. Industry classification: Please classify this document into ONE of these categories:
   - Planting (crop cultivation, agriculture, farming)
   - Livestock (animal husbandry, cattle, poultry)
   - Inputs-Soil (fertilizer, soil testing, agricultural inputs)
   - Agri-Finance (agricultural finance, insurance, credit)
   - SupplyChain-Storage (supply chain, logistics, warehouse)
   - Climate-RemoteSensing (climate, weather, remote sensing, NDVI, EVI)
   - Agri-IoT (IoT sensors, irrigation, smart agriculture)
   If none of these categories fit, respond with "Unclassified"
3. Key information extraction
4. File summary (within 200 words)

IMPORTANT: Please clearly state the industry classification in your response, for example: "Industry Classification: Planting" or "Industry: Livestock"

Please answer in English, with clear and organized format."""

                # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶ï¼ˆå¢åŠ åˆ°8000ä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
                extracted_text_limited = extracted_text[:8000]
                user_prompt = f"""Please analyze the following file content:

{extracted_text_limited}

Please provide detailed analysis results, and clearly state the Industry Classification. Make sure to provide a complete and comprehensive analysis."""

                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]

                # è°ƒç”¨DeepSeek API - å¢åŠ max_tokensä»¥ç¡®ä¿å®Œæ•´å“åº”
                ai_analysis = self.call_deepseek_api(messages, max_tokens=4000, temperature=0.7)
                
                if ai_analysis:
                    # è§£æAIè¿”å›çš„åˆ†æç»“æœ
                    # é¦–å…ˆå°è¯•ä»AIå“åº”ä¸­æå–è¡Œä¸šåˆ†ç±»
                    classification = self._extract_classification_from_ai_response(ai_analysis, extracted_text)
                    
                    # å¦‚æœæ— æ³•ä»AIå“åº”ä¸­æå–ï¼Œä½¿ç”¨æœ¬åœ°åˆ†ç±»æ–¹æ³•
                    if not classification or classification.get('category') == 'Unclassified' or classification.get('category') == 'æœªåˆ†ç±»':
                        print(f"[DEBUG] analyze_file_with_ai: æ— æ³•ä»AIå“åº”ä¸­æå–åˆ†ç±»ï¼Œä½¿ç”¨æœ¬åœ°åˆ†ç±»æ–¹æ³•")
                        classification = self.classify_industry(extracted_text)
                    
                    if isinstance(classification, dict) and 'category' in classification:
                        classification['category'] = self._to_english_category(classification['category'])
                        print(f"[DEBUG] analyze_file_with_ai: æœ€ç»ˆåˆ†ç±»ç»“æœ: {classification['category']}, ç½®ä¿¡åº¦: {classification.get('confidence', 0)}")

                    # æå–å…³é”®çŸ­è¯­ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
                    key_phrases = self.extract_key_phrases(extracted_text)

                    # ä½¿ç”¨AIç”Ÿæˆçš„æ‘˜è¦ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æœ¬åœ°ç”Ÿæˆ
                    summary = ai_analysis[:200] if ai_analysis else self.generate_summary(extracted_text)
                    
                    # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
                    conn = sqlite3.connect(self.db_path)
                    cursor = conn.cursor()

                    cursor.execute('''
                        INSERT INTO ai_analysis (file_id, analysis_type, industry_category, extracted_text, key_phrases, summary, confidence_score, method, ocr_content)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (file_id, "full_analysis", classification["category"],
                          extracted_text[:1000], json.dumps(key_phrases, ensure_ascii=False),
                          summary, classification["confidence"], "DeepSeek AI", ocr_content))

                    conn.commit()
                    conn.close()

                    return {
                        "success": True,
                        "extracted_text": extracted_text,
                        "classification": classification,
                        "key_phrases": key_phrases,
                        "summary": summary,
                        "ai_analysis": ai_analysis
                    }
                else:
                    # DeepSeek APIè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°åˆ†æ
                    st.warning("DeepSeek API call failed, using local analysis method")
            
            # å›é€€åˆ°æœ¬åœ°åˆ†ææ–¹æ³•
            classification = self.classify_industry(extracted_text)
            if isinstance(classification, dict) and 'category' in classification:
                classification['category'] = self._to_english_category(classification['category'])

            key_phrases = self.extract_key_phrases(extracted_text)
            summary = self.generate_summary(extracted_text)

            # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute('''
                INSERT INTO ai_analysis (file_id, analysis_type, industry_category, extracted_text, key_phrases, summary, confidence_score, method, ocr_content)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (file_id, "full_analysis", classification["category"],
                  extracted_text[:1000], json.dumps(key_phrases, ensure_ascii=False),
                  summary, classification["confidence"], "Local Analysis", ocr_content))

            conn.commit()
            conn.close()

            return {
                "success": True,
                "extracted_text": extracted_text,
                "classification": classification,
                "key_phrases": key_phrases,
                "summary": summary
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    def get_ai_analysis(self, file_id: int) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡ä»¶çš„AIåˆ†æç»“æœ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT analysis_type, industry_category, extracted_text, key_phrases, summary, confidence_score, method, analysis_time
            FROM ai_analysis WHERE file_id = ? ORDER BY analysis_time DESC LIMIT 1
        ''', (file_id,))
        result = cursor.fetchone()
        conn.close()

        if result:
            analysis_type, industry_category, extracted_text, key_phrases, summary, confidence_score, method, analysis_time = result
            return {
                "analysis_type": analysis_type,
                "industry_category": industry_category,
                "extracted_text": extracted_text,
                "key_phrases": json.loads(key_phrases) if key_phrases else [],
                "summary": summary,
                "confidence_score": confidence_score,
                "method": method or "Unknown",
                "analysis_time": analysis_time
            }
        return None

    def create_industry_folder(self, category: str) -> int:
        """ä¸ºè¡Œä¸šåˆ†ç±»åˆ›å»ºæ–‡ä»¶å¤¹"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å·²å­˜åœ¨ï¼ˆè‹±æ–‡å‘½åï¼‰
        eng_category = self._to_english_category(category)
        cursor.execute('SELECT id FROM folders WHERE folder_name = ?', (f"AI_{eng_category}",))
        result = cursor.fetchone()

        if result:
            folder_id = result[0]
        else:
            cursor.execute('''
                INSERT INTO folders (folder_name, parent_folder_id)
                VALUES (?, ?)
            ''', (f"AI_{eng_category}", None))
            folder_id = cursor.lastrowid

        conn.commit()
        conn.close()
        return folder_id

    def move_file_to_industry_folder(self, file_id: int, category: str) -> Dict[str, Any]:
        """å°†æ–‡ä»¶ç§»åŠ¨åˆ°è¡Œä¸šåˆ†ç±»æ–‡ä»¶å¤¹
        
        Returns:
            DictåŒ…å«successå’Œfolder_idï¼ˆå¦‚æœæˆåŠŸï¼‰
        """
        try:
            # å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_info = self.get_file_by_id(file_id)
            if not file_info:
                print(f"[DEBUG] move_file_to_industry_folder: æ–‡ä»¶ä¸å­˜åœ¨ - file_id: {file_id}")
                return {"success": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}
            
            print(f"[DEBUG] move_file_to_industry_folder: å¼€å§‹ç§»åŠ¨æ–‡ä»¶ - file_id: {file_id}, category: {category}")
            
            # åˆ›å»ºæˆ–è·å–åˆ†ç±»æ–‡ä»¶å¤¹
            folder_id = self.create_industry_folder(category)
            print(f"[DEBUG] move_file_to_industry_folder: æ–‡ä»¶å¤¹ID: {folder_id}")

            # æ›´æ–°æ–‡ä»¶çš„folder_id
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('UPDATE files SET folder_id = ? WHERE id = ?', (folder_id, file_id))
            affected_rows = cursor.rowcount
            conn.commit()
            conn.close()
            
            if affected_rows > 0:
                print(f"[DEBUG] move_file_to_industry_folder: âœ… æ–‡ä»¶ç§»åŠ¨æˆåŠŸ - file_id: {file_id}, folder_id: {folder_id}")
                return {"success": True, "folder_id": folder_id, "category": category}
            else:
                print(f"[DEBUG] move_file_to_industry_folder: âš ï¸ æœªæ›´æ–°ä»»ä½•è¡Œ - file_id: {file_id}")
                return {"success": False, "error": "æ–‡ä»¶ç§»åŠ¨å¤±è´¥ï¼Œæœªæ›´æ–°ä»»ä½•è®°å½•"}
        except Exception as e:
            print(f"[DEBUG] move_file_to_industry_folder: âŒ é”™è¯¯: {str(e)}")
            import traceback
            print(f"[DEBUG] move_file_to_industry_folder: é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            return {"success": False, "error": f"æ–‡ä»¶ç§»åŠ¨å¤±è´¥: {str(e)}"}

    # ==================== åŸºç¡€æ–‡ä»¶ç®¡ç†åŠŸèƒ½ ====================

    def rename_file(self, file_id: int, new_filename: str) -> Dict[str, Any]:
        """é‡å‘½åæ–‡ä»¶"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # æ£€æŸ¥æ–°æ–‡ä»¶åæ˜¯å¦å·²å­˜åœ¨
            cursor.execute('SELECT id FROM files WHERE filename = ? AND id != ?', (new_filename, file_id))
            if cursor.fetchone():
                conn.close()
                return {"success": False, "error": "æ–‡ä»¶åå·²å­˜åœ¨"}

            # æ›´æ–°æ–‡ä»¶å
            cursor.execute('UPDATE files SET filename = ? WHERE id = ?', (new_filename, file_id))
            conn.commit()
            conn.close()

            return {"success": True, "new_filename": new_filename}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def delete_file(self, file_id: int) -> Dict[str, Any]:
        """åˆ é™¤æ–‡ä»¶"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # è·å–æ–‡ä»¶è·¯å¾„
            cursor.execute('SELECT file_path FROM files WHERE id = ?', (file_id,))
            result = cursor.fetchone()

            if result:
                file_path = result[0]

                # åˆ é™¤ç‰©ç†æ–‡ä»¶
                if os.path.exists(file_path):
                    os.remove(file_path)

                # åˆ é™¤æ•°æ®åº“è®°å½•
                cursor.execute('DELETE FROM files WHERE id = ?', (file_id,))

                # åˆ é™¤AIåˆ†æè®°å½•
                cursor.execute('DELETE FROM ai_analysis WHERE file_id = ?', (file_id,))

                conn.commit()
                conn.close()

                return {"success": True}
            else:
                conn.close()
                return {"success": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def rename_folder(self, folder_id: int, new_folder_name: str) -> Dict[str, Any]:
        """é‡å‘½åæ–‡ä»¶å¤¹"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # æ£€æŸ¥æ–°æ–‡ä»¶å¤¹åæ˜¯å¦å·²å­˜åœ¨
            cursor.execute('SELECT id FROM folders WHERE folder_name = ? AND id != ?', (new_folder_name, folder_id))
            if cursor.fetchone():
                conn.close()
                return {"success": False, "error": "æ–‡ä»¶å¤¹åå·²å­˜åœ¨"}

            # æ›´æ–°æ–‡ä»¶å¤¹å
            cursor.execute('UPDATE folders SET folder_name = ? WHERE id = ?', (new_folder_name, folder_id))
            conn.commit()
            conn.close()

            return {"success": True, "new_folder_name": new_folder_name}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def delete_folder(self, folder_id: int) -> Dict[str, Any]:
        """åˆ é™¤æ–‡ä»¶å¤¹"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©º
            cursor.execute('SELECT COUNT(*) FROM files WHERE folder_id = ?', (folder_id,))
            file_count = cursor.fetchone()[0]

            if file_count > 0:
                conn.close()
                return {"success": False, "error": f"æ–‡ä»¶å¤¹ä¸ä¸ºç©ºï¼ŒåŒ…å« {file_count} ä¸ªæ–‡ä»¶"}

            # æ£€æŸ¥æ˜¯å¦æœ‰å­æ–‡ä»¶å¤¹
            cursor.execute('SELECT COUNT(*) FROM folders WHERE parent_folder_id = ?', (folder_id,))
            subfolder_count = cursor.fetchone()[0]

            if subfolder_count > 0:
                conn.close()
                return {"success": False, "error": f"æ–‡ä»¶å¤¹åŒ…å« {subfolder_count} ä¸ªå­æ–‡ä»¶å¤¹"}

            # åˆ é™¤æ–‡ä»¶å¤¹
            cursor.execute('DELETE FROM folders WHERE id = ?', (folder_id,))
            conn.commit()
            conn.close()

            return {"success": True}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def get_folders(self, parent_folder_id: Optional[int] = None) -> List[Dict[str, Any]]:
        """è·å–æ–‡ä»¶å¤¹åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        if parent_folder_id is None:
            cursor.execute('''
                SELECT id, folder_name, created_time, 
                       (SELECT COUNT(*) FROM files WHERE folder_id = folders.id) as file_count
                FROM folders 
                WHERE parent_folder_id IS NULL
                ORDER BY created_time DESC
            ''')
        else:
            cursor.execute('''
                SELECT id, folder_name, created_time,
                       (SELECT COUNT(*) FROM files WHERE folder_id = folders.id) as file_count
                FROM folders 
                WHERE parent_folder_id = ?
                ORDER BY created_time DESC
            ''', (parent_folder_id,))

        folders = []
        for row in cursor.fetchall():
            folders.append({
                "id": row[0],
                "folder_name": row[1],
                "created_time": row[2],
                "file_count": row[3]
            })

        conn.close()
        return folders

    def sync_cached_files(self) -> Dict[str, Any]:
        """åŒæ­¥ç¼“å­˜æ–‡ä»¶åˆ°äº‘ç«¯"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # è·å–æ‰€æœ‰å·²ç¼“å­˜çš„æ–‡ä»¶
            cursor.execute('''
                SELECT id, filename, file_path, last_modified
                FROM files 
                WHERE is_cached = TRUE
            ''')

            cached_files = cursor.fetchall()
            synced_count = 0

            for file_id, filename, file_path, last_modified in cached_files:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
                if os.path.exists(file_path):
                    # æ›´æ–°æœ€åä¿®æ”¹æ—¶é—´
                    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    cursor.execute('''
                        UPDATE files 
                        SET last_modified = ? 
                        WHERE id = ?
                    ''', (current_time, file_id))
                    synced_count += 1

            conn.commit()
            conn.close()

            return {
                "success": True,
                "synced_count": synced_count,
                "message": f"æˆåŠŸåŒæ­¥ {synced_count} ä¸ªç¼“å­˜æ–‡ä»¶"
            }
        except Exception as e:
            return {"success": False, "error": str(e)}


# åˆå§‹åŒ–äº‘å­˜å‚¨ç®¡ç†å™¨
